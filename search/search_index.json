{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"\ud83e\uddea MetaboT \ud83c\udf75  <p>Leveraging AI and knowledge graph for metabolomics analysis</p> Get Started"},{"location":"#welcome-to-metabot","title":"Welcome to \ud83e\uddea MetaboT \ud83c\udf75 \ud83d\ude80","text":"<p>Take a break, brew a cup of tea \ud83c\udf75, and let \ud83e\uddea MetaboT \ud83c\udf75 dig into your mass spec data! While you enjoy your favorite brew, our AI system will be busy infusing your data with meaning. Sip, smile, and watch the insights steep into brilliance! \u2728</p> <p>\ud83d\udc49 Try the MetaboT Web App: https://metabot.holobiomicslab.eu</p>"},{"location":"#key-features","title":"Key Features \u2728","text":"<ul> <li>AI-Powered Analysis : Utilizes LangChain and various AI agents for intelligent data processing.</li> <li>Graph-Based Architecture : Built on robust graph management systems for complex data relationships.</li> <li>SPARQL Integration : Advanced querying capabilities for metabolomics data.</li> <li>Extensible Framework : Modular design allowing for easy extension and customization.</li> </ul>"},{"location":"#project-overview","title":"Project Overview \ud83d\udd2c","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 is designed to help researchers and scientists in:</p> <ul> <li>Processing and analyzing metabolomics data.</li> <li>Identifying complex patterns and relationships.</li> <li>Generating insights from metabolomics experiments.</li> <li>Managing and querying large-scale metabolomics datasets.</li> </ul>"},{"location":"#quick-links","title":"Quick Links \ud83d\udd17","text":"<ul> <li>Installation Guide </li> <li>Quick Start Tutorial </li> <li>API Reference </li> <li>Example Usage </li> </ul>"},{"location":"#architecture","title":"Architecture \ud83c\udfd7\ufe0f","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 is built with a modular architecture consisting of several key components:</p> <pre><code>graph TD\n    A[Entry Agent] --&gt; B[Validator Agent]\n    B --&gt; C[Supervisor Agent]\n    C &lt;--&gt; D[ENPKG Agent]\n    C &lt;--&gt; E[SPARQL Agent]\n    C &lt;--&gt; F[Interpreter Agent]\n\n    E --&gt; G[Knowledge Graph]\n\n</code></pre>"},{"location":"#contributing","title":"Contributing \ud83e\udd1d","text":"<p>We welcome contributions! Please see our Contributing Guide for details on how to:</p> <ul> <li>Submit bug reports and feature requests</li> <li>Contribute code changes</li> <li>Improve documentation</li> <li>Participate in the community</li> </ul>"},{"location":"#license","title":"License \ud83d\udcdc","text":"<p>This project is licensed under the Apache License - see the LICENSE file for details.</p>"},{"location":"#citation","title":"Citation \ud83d\udd16","text":"<p>If you use \ud83e\uddea MetaboT \ud83c\udf75 in your research, please cite as follows:</p> <p>\ud83e\uddea MetaboT \ud83c\udf75: An LLM-based Multi-Agent Framework for Interactive Analysis of Mass Spectrometry Metabolomics Knowledge Madina Bekbergenova, Lucas Pradi, Benjamin Navet, Emma Tysinger, Matthieu Feraud, Yousouf Taghzouti, Martin Legrand, Tao Jiang, Franck Michel, Yan Zhou Chen, Soha Hassoun, Olivier Kirchhoffer, Jean-Luc Wolfender, Florence Mehl, Marco Pagni, Wout Bittremieux, Fabien Gandon, Louis-F\u00e9lix Nothias. PREPRINT (Version 1) available at Research Square </p> <p>Institutions:</p> <ul> <li>Universit\u00e9 C\u00f4te d'Azur, CNRS, ICN, Nice, France</li> <li>Interdisciplinary Institute for Artificial Intelligence (3iA) C\u00f4te d'Azur, Sophia-Antipolis, France</li> <li>Department of Computer Science, University of Antwerp, Antwerp, Belgium</li> <li>Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA, USA</li> <li>INRIA, Universit\u00e9 C\u00f4te d'Azur, CNRS, I3S, France</li> <li>Department of Computer Science, Tufts University, Medford, MA 02155, USA</li> <li>Department of Chemical and Biological Engineering, Tufts University, Medford, MA 02155, USA</li> <li>Institute of Pharmaceutical Sciences of Western Switzerland, University of Geneva, Centre M\u00e9dical Universitaire, Geneva, Switzerland</li> <li>School of Pharmaceutical Sciences, University of Geneva, Centre M\u00e9dical Universitaire, Geneva, Switzerland</li> <li>Swiss Institute of Bioinformatics (SIB), Lausanne, Switzerland</li> </ul>"},{"location":"contributing/","title":"Contributing to \ud83e\uddea MetaboT \ud83c\udf75 \ud83d\udcdd","text":"<p>We appreciate your interest in contributing to \ud83e\uddea MetaboT \ud83c\udf75! Below are the guidelines to help you get started.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute \ud83e\udd1d","text":""},{"location":"contributing/#fork-the-repository","title":"Fork the Repository","text":"<p>Fork the MetaboT repository to your GitHub account.</p> <p>Clone your forked repository to your local machine:</p> <ul> <li>Fork the repository on GitHub.</li> <li>Clone your fork and check out a new branch from the <code>dev</code> branch.</li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/MetaboT.git\ncd MetaboT\ngit checkout -b dev\n</code></pre>"},{"location":"contributing/#create-a-branch","title":"Create a Branch","text":"<p>Create a new branch for your feature or bugfix. For example, if you're working on a new feature, you might create a branch off the <code>dev</code> branch.</p>"},{"location":"contributing/#development-process","title":"Development Process","text":"<p>Making Changes</p> <ul> <li>Make your changes ensuring all references to files (e.g., configuration files like app/config/params.ini) are updated as needed.</li> <li>Commit your changes with clear, meaningful commit messages.</li> <li>Push your feature branch and open a pull request against the <code>dev</code> branch.</li> </ul>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":"<ul> <li>Follow the existing code style (Google DocString).</li> <li>Write clear and concise commit messages.</li> <li>Include comments and docstrings where necessary.</li> </ul>"},{"location":"contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Follow PEP8 for Python code. See the Python Style Guide (PEP 8).</li> <li>Include detailed documentation and inline comments where applicable.</li> </ul>"},{"location":"contributing/#tests","title":"Tests","text":"<p>You can find our test suite in the app/core/tests/ directory.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Update the documentation to reflect your changes. This includes:</p> <ul> <li>Docstrings in the code.</li> <li> <p>Relevant Markdown files in the docs/ directory, including:</p> <ul> <li>API Reference</li> <li>User Guide</li> <li>Examples</li> <li>Getting Started</li> </ul> </li> </ul>"},{"location":"contributing/#submitting-your-changes","title":"Submitting Your Changes \ud83d\udce4","text":""},{"location":"contributing/#commit-your-changes","title":"Commit Your Changes","text":"<p>Commit your changes with a descriptive message:</p> <pre><code>git add .\ngit commit -m \"Add new feature X\"\n</code></pre>"},{"location":"contributing/#push-to-your-fork","title":"Push to Your Fork","text":"<p>Push your changes to your forked repository:</p> <pre><code>git push origin my-feature-branch\n</code></pre>"},{"location":"contributing/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Open a pull request from your branch to the <code>dev</code> branch of the original repository. Provide a clear description of your changes and any relevant information.</p>"},{"location":"contributing/#code-review","title":"Code Review","text":"<p>Your pull request to the <code>dev</code> branch will be reviewed by an AI-agent and then by the maintainers. They may request changes or provide feedback. Please be responsive and address any comments or suggestions.</p>"},{"location":"contributing/#community","title":"Community \ud83d\udc65","text":"<ul> <li>You are welcome to use, reuse, and enrich \ud83e\uddea MetaboT \ud83c\udf75.</li> <li>Be respectful and considerate in your interactions.</li> <li>Help others and share your knowledge.</li> <li>Check our examples for guidance.</li> </ul>"},{"location":"contributing/#additional-resource","title":"Additional Resource \ud83d\udcda","text":"<ul> <li>Writing Good Commit Messages</li> </ul> <p>Thank you for contributing to \ud83e\uddea MetaboT \ud83c\udf75! Your efforts help make this project better for everyone.</p>"},{"location":"api-reference/agents/","title":"Agents API Reference \ud83e\udd16","text":"<p>This document details the agent system in the application, including the specialized agents and their roles in processing queries.</p>"},{"location":"api-reference/agents/#common-architecture","title":"Common Architecture","text":"<p>All agents in the system share a similar architectural pattern and are managed by the agent factory (<code>app.core.agents.agents_factory</code>):</p> <p>Core Components:</p> <ul> <li>Dynamic Agent Creation: Loads agent modules based on configuration settings</li> <li>Flexible Parameter Handling: Uses introspection to pass only required parameters to each agent</li> <li>LLM Selection: Supports configuration-based and agent-specific LLM selection</li> <li>Session Management: Maintains consistent session IDs across the agent ecosystem</li> <li>Error Handling: Provides robust logging and exception handling</li> </ul>"},{"location":"api-reference/agents/#core-factory-function","title":"Core Factory Function","text":"<p>The <code>create_all_agents</code> function serves as the entry point for initializing the entire agent ecosystem:</p> <pre><code>def create_all_agents(llms, graph, openai_key=None, session_id=None):\n    \"\"\"\n    Dynamically create and initialize all agent modules as specified in the configuration.\n\n    Parameters:\n        llms (dict): A dictionary mapping LLM keys to their instances.\n        graph: The graph instance used by the agents.\n        openai_key (str, optional): The OpenAI API key to be used by agents.\n        session_id (str, optional): A unique session identifier.\n\n    Returns:\n        dict: A dictionary mapping agent names to their created executor instances.\n    \"\"\"\n</code></pre>"},{"location":"api-reference/agents/#individual-agent-architecture","title":"Individual Agent Architecture","text":"<p>Each agent in the system follows a consistent structural pattern while maintaining specialized functionality.</p>"},{"location":"api-reference/agents/#common-agent-components","title":"Common Agent Components","text":"<ul> <li>Creation Function: Each agent implements a <code>create_agent</code> function that returns an <code>AgentExecutor</code></li> <li>Tool Management: Dynamically loads tools from its directory using the <code>import_tools</code> utility</li> <li>Role-Specific Prompts: Defines behavior through customized prompts</li> <li>LLM Integration: Utilizes language models as reasoning engines</li> <li>Logging: Implements consistent logging for monitoring and debugging</li> </ul>"},{"location":"api-reference/agents/#standard-agent-structure","title":"Standard Agent Structure","text":"<pre><code>def create_agent(llms, graph, openai_key, llm_instance=None) -&gt; AgentExecutor:\n    \"\"\"\n    Creates and configures an agent with its specialized tools.\n\n    Parameters:\n        llms (dict): Available language models.\n        graph: The knowledge graph instance.\n        openai_key (str): API key for OpenAI services.\n        llm_instance: Optional specific LLM instance to use.\n\n    Returns:\n        AgentExecutor: A configured agent executor instance.\n    \"\"\"\n    # Load tools dynamically from the agent's directory\n    # Configure the agent with appropriate prompts\n    # Return an AgentExecutor instance\n</code></pre> <p>Agent Locations:</p> <ul> <li>ENPKG Agent: <code>app/core/agents/enpkg/agent.py</code></li> <li>Entry Agent: <code>app/core/agents/entry/agent.py</code></li> <li>Interpreter Agent: <code>app/core/agents/interpreter/agent.py</code></li> <li>SPARQL Agent: <code>app/core/agents/sparql/agent.py</code></li> <li>Validator Agent: <code>app/core/agents/validator/agent.py</code></li> <li>Supervisor Agent: <code>app/core/agents/supervisor/agent.py</code></li> </ul>"},{"location":"api-reference/agents/#entry-agent","title":"Entry Agent \ud83d\udeaa","text":"<p>The Entry Agent serves as the first point of contact for user interactions.</p> <p>Purpose:</p> <ul> <li>Initial query processing and classification</li> <li>File analysis for submitted documents</li> </ul> <p>Key Features:</p> <ul> <li>Classifies queries into \"New Knowledge Question\" or \"Help me understand Question\"</li> <li>Analyzes submitted files using the FILE_ANALYZER tool</li> <li>Validates query completeness and requests clarification when needed</li> </ul> <p>Tools:</p> <ul> <li>FILE_ANALYZER: Processes and analyzes submitted files, providing file paths and content summaries</li> </ul> <p>Usage Cases:</p> <ul> <li>When users submit new queries requiring database information</li> <li>When files need to be analyzed</li> <li>For follow-up questions requiring context from previous conversations</li> </ul>"},{"location":"api-reference/agents/#validator-agent","title":"Validator Agent \u2705","text":"<p>The Validator Agent ensures query validity and data quality.</p> <p>Purpose:</p> <ul> <li>Validates user queries against database capabilities</li> <li>Ensures data quality and consistency</li> <li>Provides feedback for invalid queries</li> </ul> <p>Validation Checks:</p> <ul> <li>Plant name verification in database</li> <li>Query compatibility with schema</li> <li>Content relevance to available nodes/entities</li> </ul> <p>Tools:</p> <ul> <li>PLANT_DATABASE_CHECKER: Verifies plant names in database</li> </ul> <p>Validation Criteria:</p> <ul> <li>Plant-specific and feature-related queries</li> <li>Grouping, counting, and annotation comparisons</li> <li>Schema compatibility</li> <li>Data availability</li> </ul>"},{"location":"api-reference/agents/#supervisor-agent","title":"Supervisor Agent \ud83d\udc68\u200d\ud83d\udcbc","text":"<p>The Supervisor Agent orchestrates the interaction between all other agents in the system.</p> <p>Purpose:</p> <ul> <li>Coordinates information flow between agents</li> <li>Makes routing decisions based on query content</li> <li>Ensures proper processing sequence</li> <li>Manages agent responses and task completion</li> </ul> <p>Decision Making:</p> <ul> <li>Routes queries containing entities (compounds, taxa, targets) to ENPKG Agent</li> <li>Forwards resolved entities and queries to SPARQL Agent</li> <li>Directs query results to Interpreter Agent when needed</li> <li>Determines when to complete the process</li> </ul>"},{"location":"api-reference/agents/#enpkg-agent","title":"ENPKG Agent \ud83e\uddec","text":"<p>The ENPKG Agent specializes in resolving and standardizing entities mentioned in queries.</p> <p>Purpose:</p> <ul> <li>Resolves entity references to standardized identifiers</li> <li>Handles multiple types of entities (chemicals, taxa, targets)</li> <li>Provides unit information for numerical values</li> </ul> <p>Tools:</p> <ul> <li>CHEMICAL_RESOLVER: Maps chemical names to NPC Class URIs</li> <li>TAXON_RESOLVER: Resolves taxonomic names to Wikidata IRIs</li> <li>TARGET_RESOLVER: Maps target names to ChEMBLTarget IRIs</li> <li>SMILE_CONVERTER: Converts SMILE structures to InChIKey notation</li> </ul> <p>Entity Types Handled:</p> <ul> <li>Natural product compounds (with chemical class identifiers)</li> <li>Taxonomic names (with Wikidata IRIs)</li> <li>Molecular targets (with ChEMBL IRIs)</li> <li>Chemical structures (in SMILE notation)</li> </ul>"},{"location":"api-reference/agents/#sparql-agent","title":"SPARQL Agent \ud83d\udd0e","text":"<p>The SPARQL Agent handles the generation and execution of database queries.</p> <p>Purpose:</p> <ul> <li>Generates and executes SPARQL queries</li> <li>Handles query optimization and improvement</li> <li>Manages data retrieval and formatting</li> </ul> <p>Key Components:</p> <ul> <li>Query Generation Chain: Creates initial SPARQL queries</li> <li>Query Improvement Chain: Refines queries if initial results are empty</li> <li>Vector Search: Finds similar successful queries for improvement</li> <li>Schema Validation: Ensures queries follow database schema</li> </ul> <p>Tools:</p> <ul> <li>SPARQL_QUERY_RUNNER: Executes SPARQL queries against the knowledge graph</li> <li>WIKIDATA_QUERY_TOOL: Retrieves data from Wikidata in specific case</li> <li>OUTPUT_MERGE: Combines results from multiple sources</li> </ul> <p>Query Processing Features:</p> <ul> <li>Automatic query improvement when no results are found</li> <li>Token management for large result sets</li> <li>Results are automatically saved as temporary CSV files in the user's session directory</li> <li>Local CSV storage enables users to perform additional analysis or processing on the results</li> </ul>"},{"location":"api-reference/agents/#interpreter-agent","title":"Interpreter Agent \ud83d\udcca","text":"<p>The Interpreter Agent processes and formats query results for human understanding.</p> <p>Purpose:</p> <ul> <li>Processes SPARQL query outputs</li> <li>Handles file interpretation</li> <li>Generates visualizations</li> <li>Provides clear, formatted answers</li> </ul> <p>Input Processing: - Handles SPARQL query results - Processes user-submitted files - Interprets data files</p> <p>Tools:</p> <ul> <li>INTERPRETER_TOOL: Main tool for data interpretation and visualization</li> <li>SPECTRUM_PLOTTER: Provides the url with a plot of a spectrum given its USI</li> </ul> <p>Output Types:</p> <ul> <li>Text interpretations of query results</li> <li>Generated visualizations</li> <li>Formatted data summaries</li> <li>File path references for downloads</li> <li>URL which connects to the Metabolomics Spectrum Resolver tool</li> </ul>"},{"location":"api-reference/agents/#agent-interaction-architecture","title":"Agent Interaction Architecture \ud83d\udd04","text":"<p>The agent system is implemented using LangGraph's StateGraph, where agents operate as nodes in a directed graph with state-managed transitions.</p> <p>Core Architecture: <pre><code>class AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]  # Accumulated messages\n    next: str  # Next routing target\n</code></pre></p> <p>Workflow Components:</p> <ol> <li> <p>Graph Structure:</p> <ul> <li>Entry_Agent as the designated entry point</li> <li>Nodes representing individual agents</li> <li>Conditional edges defining valid transitions</li> <li>State preservation across transitions</li> </ul> </li> <li> <p>Routing Logic:</p> <ul> <li>Entry_Agent can route to Supervisor or Validator</li> <li>Validator makes routing decisions based on query validation</li> <li>Supervisor dynamically routes to specialized agents</li> <li>Process ends when valid response generated or error detected</li> </ul> </li> <li> <p>State Management:</p> <ul> <li>Messages accumulate throughout processing</li> <li>Each agent adds its output to message history</li> <li>State maintained across all transitions</li> <li>Metadata preserved for context</li> </ul> </li> </ol> <p>Communication Patterns:</p> <ol> <li> <p>Primary Flow:    <pre><code>Entry_Agent \u2192 Validator \u2192 Supervisor \u2192 [Specialized Agents] \u2192 __end__\n</code></pre></p> </li> <li> <p>Conditional Branching:</p> <ul> <li>Validator routes based on query validity</li> <li>Supervisor routes based on query content analysis</li> <li>Dynamic routing to specialized agents as needed</li> </ul> </li> <li> <p>Message Handling:</p> <ul> <li>Human messages initiate workflow</li> <li>Agent responses added to message sequence</li> <li>State updates trigger next agent selection</li> <li>Final response terminates workflow</li> </ul> </li> </ol> <p>This architecture ensures:</p> <ul> <li>Clear communication paths</li> <li>Stateful processing</li> <li>Dynamic routing</li> <li>Error handling</li> <li>Process monitoring</li> </ul>"},{"location":"api-reference/agents/#agent-setup-guidelines","title":"Agent Setup Guidelines \ud83e\uddd1\u200d\ud83d\udcbb","text":""},{"location":"api-reference/agents/#agent-directory-creation","title":"Agent Directory Creation","text":"<p>Create a dedicated folder for your agent within the <code>app/core/agents/</code> directory.</p>"},{"location":"api-reference/agents/#standard-file-structure","title":"Standard File Structure","text":"<ul> <li> <p>Agent (<code>agent.py</code>): Copy from an existing agent unless your tool requires private class property access. Refer to \"If Your Tool Serves as an Agent\" for special cases.</p> <p>Psst... don't let the complexities of Python imports overcomplicate your flow\u2014trust the process!</p> </li> <li> <p>Prompt (<code>prompt.py</code>): Adapt the prompt for your specific context/tasks.</p> </li> <li> <p>Tools (<code>tool_xxxx.py</code>) (optional): Inherit from the LangChain <code>BaseTool</code>, defining:</p> <ul> <li><code>name</code>, <code>description</code>, <code>args_schema</code></li> <li>A Pydantic model for input validation</li> <li>The <code>_run</code> method for execution</li> </ul> </li> </ul>"},{"location":"api-reference/agents/#supervisor-configuration","title":"Supervisor Configuration","text":"<p>Modify the supervisor prompt (see supervisor prompt) to detect and select your agent. Our AI PR-Agent \ud83e\udd16 is triggered automatically through issues and pull requests, so you'll be in good hands!</p>"},{"location":"api-reference/agents/#configuration-updates","title":"Configuration Updates","text":"<p>Update <code>app/config/langgraph.json</code> to include your agent in the workflow and specify <code>llm_choice</code> based on the models defined in <code>app/config/params.ini</code>. Available models include:</p> <ul> <li>OpenAI models: <code>llm_preview</code>, <code>llm_o</code>, <code>llm_mini</code></li> <li>OVH models: <code>ovh_Meta-Llama-3_1-70B-Instruct</code></li> <li>Deepseek models: <code>deepseek_deepseek-chat</code>, <code>deepseek_deepseek-reasoner</code></li> <li>LiteLLM compatible models: <code>llm_litellm_openai</code>, <code>llm_litellm_deepseek</code>, <code>llm_litellm_claude</code>, <code>llm_litellm_gemini</code></li> </ul> <p>Choose the appropriate model based on your agent's requirements for reasoning capabilities and performance. For reference, see langgraph.json. If you need to add a new language model, refer to the Language Model Configuration guide.</p>"},{"location":"api-reference/agents/#if-your-tool-serves-as-an-agent","title":"If Your Tool Serves as an Agent","text":"<p>For LLM-interaction, make sure additional class properties are set in <code>agent.py</code> (refer to tool_sparql.py and agent.py). Keep it snazzy and smart!</p>"},{"location":"api-reference/core/","title":"Core API Reference \u2699\ufe0f","text":"<p>This document provides detailed information about the core components and functions of \ud83e\uddea MetaboT \ud83c\udf75.</p>"},{"location":"api-reference/core/#main-module","title":"Main Module \ud83d\ude80","text":"<p>The main module (<code>app.core.main</code>) provides the primary entry points and core functionality for \ud83e\uddea MetaboT \ud83c\udf75.</p>"},{"location":"api-reference/core/#functions","title":"Functions","text":""},{"location":"api-reference/core/#get_api_key","title":"<code>get_api_key</code> \ud83d\udd11","text":"<pre><code>def get_api_key(provider: str) -&gt; Optional[str]\n</code></pre> <p>Get API key for specified provider from environment variables.</p> <p>Parameters: - <code>provider</code> (str): Provider name matching a key in API_KEY_MAPPING</p> <p>Returns: - Optional[str]: API key if found, None otherwise</p>"},{"location":"api-reference/core/#create_litellm_model","title":"<code>create_litellm_model</code> \ud83e\udd16","text":"<pre><code>def create_litellm_model(config: configparser.SectionProxy) -&gt; ChatLiteLLM\n</code></pre> <p>Create a ChatLiteLLM instance based on the model id and configuration from app/config/params.ini file. The configuration includes model parameters such as temperature, max_retries, and optional base_url/api_base settings.</p> <p>Parameters: - <code>config</code> (configparser.SectionProxy): The configuration section from params.ini that contains model settings:</p> <ul> <li>Required: \"id\" - model identifier (e.g., \"gpt-4\", \"deepseek/...\")</li> <li>Optional: \"temperature\", \"max_retries\", \"base_url\", \"api_base\"</li> </ul> <p>Returns: - <code>ChatLiteLLM</code>: Configured ChatLiteLLM instance</p>"},{"location":"api-reference/core/#llm_creation","title":"<code>llm_creation</code> \ud83e\udd16","text":"<pre><code>def llm_creation(api_key: Optional[str] = None, params_file: Optional[str] = None) -&gt; Dict[str, Union[ChatOpenAI, ChatLiteLLM]]\n</code></pre> <p>Creates and configures language model instances based on the configuration file.</p> <p>Parameters:</p> <ul> <li><code>api_key</code> (Optional[str]): OpenAI API key (optional, can be set via environment variable)</li> <li><code>params_file</code> (Optional[str]): Path to an alternate configuration file</li> </ul> <p>Returns: - Dictionary of initialized language models</p> <p>Example: <pre><code>models = llm_creation()\n# With custom config file\nmodels = llm_creation(params_file=\"custom_params.ini\")\n</code></pre></p>"},{"location":"api-reference/core/#langsmith_setup","title":"<code>langsmith_setup</code> \ud83d\udee0\ufe0f","text":"<pre><code>def langsmith_setup() -&gt; Optional[Client]\n</code></pre> <p>Configures LangSmith integration for workflow tracking and monitoring. If the environment variable LANGCHAIN_API_KEY (or LANGSMITH_API_KEY) is provided, the function enables tracing and configures the default project and endpoint. Otherwise, it disables tracing.</p> <p>Returns: - Optional[Client]: LangSmith client if setup successful, None otherwise</p> <p>For advanced configuration details, see Advanced Configuration.</p> <p>Example: <pre><code>client = langsmith_setup()  # Configures LangSmith integration if API key is provided\n</code></pre></p>"},{"location":"api-reference/core/#graph-management","title":"Graph Management \ud83d\udce1","text":"<p>The graph management module (<code>app.core.graph_management.RdfGraphCustom</code>) provides tools for interacting with the RDF knowledge graph.</p>"},{"location":"api-reference/core/#rdfgraph-class","title":"RdfGraph Class","text":"<pre><code>class RdfGraph:\n    def __init__(self, query_endpoint: str, standard: str = \"rdf\"):\n        \"\"\"\n        Initialize an RDF graph connection.\n\n        Args:\n            query_endpoint (str): SPARQL endpoint URL\n            standard (str): RDF standard to use (default: \"rdf\")\n        \"\"\"\n</code></pre> <p>Key Methods:</p> <ul> <li><code>get_schema</code>: Retrieves the graph schema</li> <li><code>execute_query</code>: Runs SPARQL queries against the graph</li> <li><code>save</code>: Persists the graph state</li> </ul>"},{"location":"api-reference/core/#workflow-management","title":"Workflow Management \ud83d\udd04","text":"<p>The workflow module (<code>app.core.workflow.langraph_workflow</code>) manages the processing pipeline.</p>"},{"location":"api-reference/core/#functions_1","title":"Functions","text":""},{"location":"api-reference/core/#create_workflow","title":"<code>create_workflow</code> \ud83c\udfd7\ufe0f","text":"<pre><code>def create_workflow(models: Dict[str, Union[ChatOpenAI, ChatLiteLLM]], endpoint_url: str, evaluation: bool = False, api_key: Optional[str] = None) -&gt; Any\n</code></pre> <p>Creates a new workflow instance with the specified configuration.</p> <p>Parameters:</p> <ul> <li><code>models</code> (Dict[str, Union[ChatOpenAI, ChatLiteLLM]]): Dictionary of language model instances</li> <li><code>endpoint_url</code> (str): The URL of the SPARQL endpoint</li> <li><code>evaluation</code> (bool): Whether to run in evaluation mode</li> <li><code>api_key</code> (Optional[str]): OpenAI API key (optional)</li> </ul> <p>Returns: - Workflow instance</p>"},{"location":"api-reference/core/#process_workflow","title":"<code>process_workflow</code> \ud83d\udd04","text":"<pre><code>def process_workflow(workflow: Any, question: str) -&gt; Any\n</code></pre> <p>Processes a query through the workflow.</p> <p>Parameters:</p> <ul> <li><code>workflow</code>: The workflow instance</li> <li><code>question</code> (str): The query to process</li> </ul> <p>Returns: - Query results</p>"},{"location":"api-reference/core/#agent-factory","title":"Agent Factory \ud83c\udfed","text":"<p>The agent factory module (<code>app.core.agents.agents_factory</code>) manages agent creation and configuration.</p>"},{"location":"api-reference/core/#functions_2","title":"Functions","text":""},{"location":"api-reference/core/#create_all_agents","title":"<code>create_all_agents</code> \ud83d\udee0\ufe0f","text":"<pre><code>def create_all_agents(models: Dict[str, ChatOpenAI], graph: RdfGraph) -&gt; Dict\n</code></pre> <p>Creates all required agents for the workflow.</p> <p>Parameters:</p> <ul> <li><code>models</code>: Dictionary of language models</li> <li><code>graph</code>: RDF graph instance</li> </ul> <p>Returns: - Dictionary of initialized agents</p>"},{"location":"api-reference/core/#utility-functions","title":"Utility Functions \ud83e\uddf0","text":"<p>The utils module (<code>app.core.utils</code>) provides common utility functions.</p>"},{"location":"api-reference/core/#functions_3","title":"Functions","text":""},{"location":"api-reference/core/#setup_logger","title":"<code>setup_logger</code> \ud83d\udcdd","text":"<pre><code>def setup_logger(name: str) -&gt; logging.Logger\n</code></pre> <p>Configures a logger instance.</p> <p>Parameters:</p> <ul> <li><code>name</code> (str): Logger name</li> </ul> <p>Returns:</p> <ul> <li>Configured logger instance</li> </ul>"},{"location":"api-reference/core/#load_config","title":"<code>load_config</code> \ud83d\udcc2","text":"<pre><code>def load_config(config_path: str) -&gt; configparser.ConfigParser\n</code></pre> <p>Loads a configuration file.</p> <p>Parameters:</p> <ul> <li><code>config_path</code> (str): Path to configuration file</li> </ul> <p>Returns:</p> <ul> <li>Parsed configuration object</li> </ul>"},{"location":"api-reference/core/#database-management","title":"Database Management \ud83d\udcbe","text":"<p>The database management module provides two specialized databases for saving the outputs of tools and agents, as well as managing workflow state. These databases are created automatically based on the deployment environment:</p> <ul> <li>When running locally: Databases are created as local files in the project directory</li> <li>When deployed in the cloud: Databases are created in the cloud environment</li> </ul>"},{"location":"api-reference/core/#tools-database","title":"Tools Database \ud83d\udd27","text":"<p>The tools database (<code>app.core.memory.tools_database</code>) manages data associated with agent tools.</p> <pre><code>def tools_database() -&gt; ToolsDatabaseManager\n</code></pre> <p>Creates or returns a singleton instance of the tools database manager.</p> <p>Features:</p> <ul> <li>Automatically discovers and tracks tool usage</li> <li>Maintains interaction history</li> <li>Stores tool-specific data</li> <li>Supports both SQLite (default) and custom database backends</li> <li>Auto-creates database based on environment (local/cloud)</li> </ul> <p>Configuration:</p> <ul> <li>Default: Creates local <code>tools_database.db</code> file when running locally</li> <li> <p>Custom: Set via environment variables:</p> <ul> <li><code>DATABASE_URL</code>: Database connection string (used for cloud deployment)</li> <li><code>TOOLS_DATABASE_MANAGER_CLASS</code>: Custom manager class path</li> </ul> </li> </ul>"},{"location":"api-reference/core/#memory-database","title":"Memory Database \ud83d\udcad","text":"<p>The memory database (<code>app.core.memory.custom_sqlite_file</code>) manages workflow state and checkpoints.</p> <pre><code>def memory_database() -&gt; SqliteCheckpointerSaver\n</code></pre> <p>Creates or returns a singleton instance of the memory database manager.</p> <p>Features:</p> <ul> <li>Saves workflow state checkpoints</li> <li>Supports multi-threading (essential for Streamlit)</li> <li>Automatic cleanup on restart</li> <li>Thread-safe operations</li> <li>Auto-creates database based on environment (local/cloud)</li> </ul> <p>Configuration:</p> <ul> <li>Default: Creates local <code>langgraph_checkpoint.db</code> file when running locally</li> <li>Custom: Set via environment variables:</li> <li><code>DATABASE_URL</code>: Database connection string (used for cloud deployment)</li> <li><code>MEMORY_DATABASE_MANAGER_CLASS</code>: Custom manager class path</li> </ul>"},{"location":"api-reference/core/#usage-examples","title":"Usage Examples \ud83d\udcd8","text":""},{"location":"api-reference/core/#basic-query-processing","title":"Basic Query Processing","text":"<pre><code>from app.core.main import link_kg_database, llm_creation\nfrom app.core.workflow.langraph_workflow import create_workflow, process_workflow\n\n# Initialize components\nendpoint_url = \"your_endpoint_url\"\nmodels = llm_creation()\n\n# Create and run workflow\nworkflow = create_workflow(\n    models=models,\n    endpoint_url=endpoint_url,\n    evaluation=False\n)\nprocess_workflow(workflow, \"Your query here\")\n</code></pre>"},{"location":"api-reference/graph-management/","title":"Graph Management API Reference \ud83d\udce1","text":"<p>This document details the graph management system in \ud83e\uddea MetaboT \ud83c\udf75, focusing on the RDF graph implementation and related utilities.</p>"},{"location":"api-reference/graph-management/#rdf-graph-custom","title":"RDF Graph Custom \ud83d\udd04","text":"<p>The <code>RdfGraphCustom</code> module provides core functionality for interacting with RDF-based knowledge graphs, with a focus on managing and querying RDF schema information. It is primarily used by the SPARQL query generation chain to execute queries against the configured endpoint.</p>"},{"location":"api-reference/graph-management/#class-rdfgraph","title":"Class: RdfGraph","text":"<p>The <code>RdfGraph</code> class handles RDF graph representation, focusing on schema management and SPARQL query execution. The class primarily works with rdfs:Class nodes and their relationships.</p>"},{"location":"api-reference/graph-management/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    query_endpoint: Optional[str],\n    standard: Optional[str] = \"rdf\",\n    schema_file: Optional[str] = None,\n    auth: Optional[Tuple[str, str]] = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>query_endpoint</code> (Optional[str]): SPARQL endpoint URL for queries (read access)</li> <li><code>standard</code> (Optional[str]): RDF standard to use - one of \"rdf\", \"rdfs\", or \"owl\" (default: \"rdf\")</li> <li><code>schema_file</code> (Optional[str]): Path to file containing RDF graph schema in turtle format</li> <li><code>auth</code> (Optional[Tuple[str, str]]): Optional authentication credentials as (username, password) tuple. If not provided, the connection will be attempted without authentication. This is useful when users need to connect to a local SPARQL endpoint that requires authentication.</li> </ul> <p>Environment Variables:</p> <ul> <li><code>SPARQL_USERNAME</code>: Username for endpoint authentication (optional)</li> <li><code>SPARQL_PASSWORD</code>: Password for endpoint authentication (optional)</li> <li><code>KG_ENDPOINT_URL</code>: URL of the SPARQL endpoint (defaults to \"https://enpkg.commons-lab.org/graphdb/repositories/ENPKG\")</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If standard is not one of rdf, rdfs, or owl</li> <li><code>ValueError</code>: If no query endpoint is provided</li> </ul>"},{"location":"api-reference/graph-management/#core-methods","title":"Core Methods","text":""},{"location":"api-reference/graph-management/#query-execution","title":"Query Execution \ud83d\ude80","text":"<pre><code>def query(self, query: str) -&gt; List[csv.DictReader]:\n    \"\"\"\n    Execute a SPARQL query against the graph.\n\n    Args:\n        query (str): SPARQL query string to execute\n\n    Returns:\n        List[csv.DictReader]: Query results as list of dictionaries\n\n    Raises:\n        ValueError: If query is invalid or execution fails\n    \"\"\"\n</code></pre>"},{"location":"api-reference/graph-management/#schema-management","title":"Schema Management \ud83c\udfd7\ufe0f","text":"<pre><code>def load_schema(self) -&gt; None:\n    \"\"\"\n    Loads graph schema information based on the specified standard (rdf, rdfs, owl).\n    The schema is either loaded from a file (if schema_file was provided) or \n    extracted from the endpoint.\n    \"\"\"\n\n@property\ndef get_schema(self) -&gt; str:\n    \"\"\"\n    Retrieve the current graph schema.\n\n    Returns:\n        str: Complete graph schema information including namespaces and node types\n    \"\"\"\n</code></pre>"},{"location":"api-reference/graph-management/#property-and-value-types","title":"Property and Value Types \ud83d\udd0d","text":"<pre><code>def get_prop_and_val_types(self, class_uri: str) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Retrieves and filters properties and their value types for a specified class URI.\n\n    Args:\n        class_uri (str): The URI of the class to analyze\n\n    Returns:\n        List[Tuple[str, str]]: List of (property_uri, value_type) pairs\n    \"\"\"\n</code></pre>"},{"location":"api-reference/graph-management/#graph-generation","title":"Graph Generation \ud83c\udf10","text":"<pre><code>def get_graph_from_classes(self, classes: List[Dict]) -&gt; rdflib.graph.Graph:\n    \"\"\"\n    Generates an RDF graph from class definitions.\n\n    Example triple:\n        ns1:InChIkey ns1:has_npc_pathway ns1:ChemicalTaxonomy .\n\n    Args:\n        classes (List[Dict]): List of class definitions\n\n    Returns:\n        rdflib.graph.Graph: Generated RDF graph\n    \"\"\"\n</code></pre>"},{"location":"api-reference/graph-management/#namespace-management","title":"Namespace Management \ud83c\udff7\ufe0f","text":"<pre><code>def get_namespaces(self) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Retrieve namespace definitions.\n\n    Returns:\n        List[Tuple[str, str]]: List of (prefix, uri) pairs\n\n    Raises:\n        ValueError: If no namespaces are found\n    \"\"\"\n</code></pre>"},{"location":"api-reference/graph-management/#usage-examples","title":"Usage Examples \ud83d\udca1","text":""},{"location":"api-reference/graph-management/#basic-graph-operations","title":"Basic Graph Operations","text":"<pre><code>from app.core.graph_management.RdfGraphCustom import RdfGraph\n\n# Initialize graph without authentication (public endpoint)\ngraph = RdfGraph(\n    query_endpoint=\"https://example.org/sparql\"\n)\n\n# Initialize graph with authentication (local/private endpoint)\ngraph = RdfGraph(\n    query_endpoint=\"http://localhost:3030/sparql\",\n    auth=(\"username\", \"password\")  # For endpoints requiring authentication\n)\n\n# Execute a simple query\nresults = graph.query(\"\"\"\n    SELECT ?s ?p ?o\n    WHERE {\n        ?s ?p ?o\n    }\n    LIMIT 10\n\"\"\")\n\n# Get schema information\nschema = graph.get_schema\n</code></pre>"},{"location":"api-reference/graph-management/#working-with-class-properties","title":"Working with Class Properties","text":"<pre><code># Get properties for a specific class\nclass_uri = \"http://example.org/onto#Compound\"\nprops = graph.get_prop_and_val_types(class_uri)\n\n# Display properties and their value types\nfor prop_uri, value_type in props:\n    print(f\"Property: {prop_uri}\")\n    print(f\"Value type: {value_type}\")\n</code></pre>"},{"location":"api-reference/graph-management/#implementation-details","title":"Implementation Details \ud83d\udd27","text":"<p>This module is designed to work as part of the larger MetaboT system. It performs the following functions:</p> <ol> <li> <p>Configuration</p> <ul> <li>Automatically connects to the specified SPARQL endpoint</li> <li>Handles authentication if credentials are provided</li> <li>Uses environment variables for flexible configuration</li> </ul> </li> <li> <p>Query Processing</p> <ul> <li>Works with the SPARQL query generation chain</li> <li>Executes generated queries against the endpoint</li> <li>Returns results in a standardized format</li> </ul> </li> <li> <p>Schema Management</p> <ul> <li>Can load schema from file or extract from endpoint</li> <li>Manages namespace definitions</li> <li>Filters system-specific properties</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Validates endpoint configuration</li> <li>Handles connection issues gracefully</li> <li>Provides clear error messages</li> </ul> </li> </ol> <p>For more detailed information about using this module within the MetaboT system, refer to the inline code documentation in RdfGraphCustom.py.</p>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples \ud83d\ude80","text":"<p>This guide provides practical examples of using \ud83e\uddea MetaboT \ud83c\udf75 for common metabolomics analysis tasks.</p>"},{"location":"examples/basic-usage/#standard-queries","title":"Standard Queries \ud83d\udd2c","text":""},{"location":"examples/basic-usage/#feature-analysis","title":"Feature Analysis","text":"<p>Count features with matching annotations:</p> <pre><code>python -m app.core.main -c \"How many features have the same SIRIUS/CSI:FingerID and ISDB annotation?\"\n</code></pre>"},{"location":"examples/basic-usage/#chemical-class-analysis","title":"Chemical Class Analysis","text":"<p>Query specific chemical classes:</p> <pre><code>python -m app.core.main -c \"Which extracts have features annotated as aspidosperma-type alkaloids by CANOPUS with a probability score above 0.5?\"\n</code></pre>"},{"location":"examples/basic-usage/#structure-identification","title":"Structure Identification","text":"<p>Get structural annotations for a specific plant:</p> <pre><code>python -m app.core.main -c \"What are the SIRIUS structural annotations for Tabernaemontana coffeoides?\"\n</code></pre>"},{"location":"examples/basic-usage/#advanced-queries","title":"Advanced Queries \u26a1\ufe0f","text":""},{"location":"examples/basic-usage/#cross-mode-feature-matching","title":"Cross-Mode Feature Matching","text":"<p>Match features across ionization modes:</p> <pre><code>python -m app.core.main -c \"Filter the pos ionization mode features of Melochia umbellata annotated as [M+H]+ by SIRIUS to keep the ones for which a feature in neg ionization mode is detected with the same retention time (+/- 3 seconds)\"\n</code></pre>"},{"location":"examples/basic-usage/#bioassay-integration","title":"Bioassay Integration","text":"<p>Query bioassay results:</p> <pre><code>python -m app.core.main -c \"List the bioassay results at 10\u00b5g/mL against T.cruzi for lab extracts of Tabernaemontana coffeoides\"\n</code></pre>"},{"location":"examples/basic-usage/#complex-analysis","title":"Complex Analysis","text":"<p>Combine multiple analysis aspects:</p> <pre><code>python -m app.core.main -c \"Which lab extracts from Melochia umbellata yield compounds that have a retention time of less than 2 minutes and demonstrate an inhibition percentage greater than 70% in bioassay results?\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic-usage","title":"Programmatic Usage \ud83d\udda5\ufe0f","text":""},{"location":"examples/basic-usage/#basic-setup","title":"Basic Setup","text":"<pre><code># Initialize components\nfrom app.core.main import llm_creation\nfrom app.core.workflow.langraph_workflow import create_workflow\n# Initialize components\nendpoint_url = \"https://enpkg.commons-lab.org/graphdb/repositories/ENPKG\"\nmodels = llm_creation()  # [llm_creation](https://github.com/nothiasl/MetaboT/blob/main/app/core/main.py)\n# Create workflow\nworkflow = create_workflow(\n    models=models,\n    endpoint_url=endpoint_url,\n    evaluation=False,\n\n)\n</code></pre>"},{"location":"examples/basic-usage/#custom-query-processing","title":"Custom Query Processing","text":"<pre><code>from app.core.workflow.langraph_workflow import process_workflow\n\n# Process a custom query\nquery = \"What are the chemical structure ISDB annotations for Lovoa trichilioides?\"\nprocess_workflow(workflow, query)  # [process_workflow](https://github.com/nothiasl/MetaboT/blob/main/app/core/workflow/langraph_workflow.py)\n</code></pre>"},{"location":"examples/basic-usage/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple queries\nqueries = [\n    \"Count the number of LCMS features in negative ionization mode\",\n    \"What are the mass spectrometry features detected for Rumex nepalensis?\",\n    \"What is the highest inhibition percentage for compounds from Rauvolfia vomitoria?\"\n]\n\nfor query in queries:\n    process_workflow(workflow, query)\n    # Process results as needed\n</code></pre>"},{"location":"examples/basic-usage/#best-practices","title":"Best Practices \ud83d\udc4d","text":"<ol> <li> <p>Query Optimization</p> <ul> <li>Be specific in your queries</li> <li>Include relevant constraints</li> <li>Consider data volume</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Close connections when done</li> <li>Monitor memory usage</li> <li>Handle large result sets appropriately</li> </ul> </li> <li> <p>Error Handling <pre><code>try:\n     process_workflow(workflow, query)\nexcept Exception as e:\n    print(f\"Error processing query: {e}\")\n    # Handle error appropriately\n</code></pre></p> </li> </ol>"},{"location":"examples/basic-usage/#common-patterns","title":"Common Patterns \ud83d\udd01","text":""},{"location":"examples/basic-usage/#feature-filtering","title":"Feature Filtering","text":"<pre><code># Filter features by retention time\nquery = \"List LC-MS features with chemical class annotation by CANOPUS and retention time between 5-7 minutes\"\n</code></pre>"},{"location":"examples/basic-usage/#annotation-comparison","title":"Annotation Comparison","text":"<pre><code># Compare annotations across methods\nquery = \"Which compounds have annotations from both ISDB and SIRIUS, and what are their molecular masses?\"\n</code></pre>"},{"location":"examples/basic-usage/#multi-criteria-analysis","title":"Multi-criteria Analysis","text":"<pre><code># Combine multiple criteria\nquery = \"Which plant has extracts containing compounds that demonstrated inhibition rates above 50% and are above 800 Da in mass?\"\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation Guide \ud83d\ude80","text":"<p>This guide will walk you through the process of installing \ud83e\uddea MetaboT \ud83c\udf75 and its dependencies.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<p>Before installing \ud83e\uddea MetaboT \ud83c\udf75, ensure you have the following installed:</p> <ul> <li>pip (Python package installer) \u2014 Install pip</li> <li>conda \u2014 Install Miniconda</li> <li>Git \u2014 Install Git</li> <li>LLM API Key \u2014 Get an API key for your chosen language model (OpenAI, DeepSeek, or Claude)</li> <li>WSL (for Windows users) \u2014 Install WSL</li> </ul>"},{"location":"getting-started/installation/#clone-the-repository-and-switch-to-the-dev-branch","title":"Clone the Repository and switch to the <code>dev</code> branch:\ud83d\udce5","text":"<pre><code>git clone https://github.com/holobiomicslab/MetaboT.git\ngit checkout dev\ncd MetaboT\n</code></pre>"},{"location":"getting-started/installation/#create-and-activate-the-conda-environment","title":"Create and Activate the Conda Environment \u2699\ufe0f","text":"<p>For macOS: <pre><code>conda env create -f environment.yml\nconda activate metabot\n</code></pre></p> <p>For Linux: <pre><code>sudo apt-get update\nsudo apt-get install -y python3-dev build-essential\nconda env create -f environment.yml\nconda activate metabot\n</code></pre></p> <p>For Windows (using WSL):</p> <ol> <li>Install WSL if you haven't already:       <pre><code>wsl --install\n</code></pre></li> <li>Open WSL and install the required packages:       <pre><code>sudo apt-get update\nsudo apt-get install -y python3-dev build-essential\n</code></pre></li> <li>Install Miniconda in WSL:       <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\n</code></pre></li> <li>Create and activate the conda environment:       <pre><code>conda env create -f environment.yml\nconda activate metabot\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#install-dependencies","title":"Install Dependencies \ud83d\udce6","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables \ud83d\udd11","text":"<p>Create a <code>.env</code> file in the root directory with the following variables:</p> <pre><code># Optional: API Keys for external services\nOPENAI_API_KEY=your_openai_api_key  # If using OpenAI service\nDEEPSEEK_API_KEY=your_deepseek_api_key # If using DeepSeek API service\nOVHCLOUD_API_KEY=your_ovhcloud_api_key # If using the OVHcloud services \n</code></pre>"},{"location":"getting-started/installation/#language-model-configuration","title":"Language Model Configuration \ud83e\udd16","text":"<p>By default, all agents in MetaboT use OpenAI models, but you can configure different models for each agent. The current implementation supports:</p> <ul> <li>OpenAI</li> <li>DeepSeek</li> <li>Claude (Anthropic)</li> <li>Llama (via OVHcloud)</li> <li>Mistral AI</li> </ul>"},{"location":"getting-started/installation/#free-model-options","title":"Free Model Options","text":"<p>You can try some free models for the agents, such as Mistral AI. To use Mistral AI:</p> <ol> <li> <p>Get your API key: </p> <ul> <li>Visit Mistral AI Platform</li> <li>Create an account or sign in</li> <li>Navigate to the API Keys section</li> <li>Generate a new API key</li> <li>For detailed API documentation, visit Mistral AI API Docs</li> </ul> </li> <li> <p>Add to your .env file:    <pre><code>MISTRAL_API_KEY=your_mistral_api_key_here\n</code></pre></p> </li> <li> <p>Configure in params.ini:    <pre><code>[llm_litellm_mistral]\ntemperature=0.0\nid=mistral/mistral-small\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#adding-new-models","title":"Adding New Models","text":"<p>To add a new model using LiteLLM, you can use any provider supported by LiteLLM. Check the LiteLLM providers documentation for the complete list of supported providers.</p> <ol> <li> <p>Add a new section in <code>app/config/params.ini</code>: <pre><code>[llm_litellm_your_model_name]\ntemperature=0.0\nid=your-provider/model-name  # As specified in https://docs.litellm.ai/docs/providers\nmax_retries=3\n</code></pre></p> </li> <li> <p>Add your provider and API key mapping in <code>app/core/main.py</code>: <pre><code>API_KEY_MAPPING = {\n    \"deepseek\": \"DEEPSEEK_API_KEY\",\n    \"ovh\": \"OVHCLOUD_API_KEY\",\n    \"openai\": \"OPENAI_API_KEY\",\n    \"huggingface\": \"HUGGINGFACE_API_KEY\",\n    \"anthropic\": \"ANTHROPIC_API_KEY\",\n    \"gemini\": \"GEMINI_API_KEY\",\n    \"your-provider\": \"YOUR_PROVIDER_API_KEY\"  # Add your mapping here\n}\n</code></pre></p> </li> <li> <p>Don't forget to add your API key to the .env file:    <pre><code>YOUR_PROVIDER_API_KEY=your_api_key_here\n</code></pre></p> </li> <li> <p>Modify the provider detection in <code>create_litellm_model</code> function: <pre><code>if model_id.startswith(\"deepseek\"):\n    provider = \"deepseek\"\nelif model_id.startswith(\"gpt\"):\n    provider = \"openai\"\n    model_name = f\"openai/{model_id}\"\nelif model_id.startswith(\"your-prefix\"):  # Add your model prefix detection\n    provider = \"your-provider\"\n</code></pre></p> </li> </ol> <p>The function automatically handles:</p> <ul> <li>Provider detection based on model ID prefix</li> <li>API key retrieval from environment variables</li> <li>Basic parameters (temperature, max_retries)</li> <li>Optional base URL configuration</li> </ul>"},{"location":"getting-started/installation/#configuring-models-for-different-agents","title":"Configuring Models for Different Agents","text":"<p>To use different models for different agents, modify <code>app/config/langgraph.json</code>. In the agents section, specify <code>llm_choice</code> with the name of your model section from params.ini:</p> <pre><code>{\n  \"agents\": [\n    {\n      \"name\": \"Entry_Agent\",\n      \"path\": \"app.core.agents.entry.agent\",\n      \"llm_choice\": \"llm_litellm_your_model_name\"\n    },\n    {\n      \"name\": \"Validator\",\n      \"path\": \"app.core.agents.validator.agent\",\n      \"llm_choice\": \"llm_litellm_different_model\"\n    }\n  ]\n}\n</code></pre> <p>Note: Currently, the LiteLLM option is not available for the Supervisor agent because it requires a specific router implementation. The Supervisor agent will continue to use the default model configuration.</p>"},{"location":"getting-started/installation/#sparql-endpoint-configuration","title":"SPARQL Endpoint Configuration \ud83c\udf10","text":"<p>Configure your SPARQL endpoint exclusively by setting the <code>KG_ENDPOINT_URL</code> variable in your <code>.env</code> file.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation \u2705","text":"<p>To verify the installation, execute the following command:</p> <pre><code>python app/tests/installation_test.py\n</code></pre> <p>This command initiates the agent workflow by constructing the RDF graph using the endpoint specified via the KG_ENDPOINT_URL variable in your .env file, instantiating the requisite language models, and executing one of the predefined standard queries. Successful execution confirms the proper configuration and integration of the system's core functionalities, including graph management and SPARQL query generation.</p>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues \ud83d\udc1e","text":""},{"location":"getting-started/installation/#issue-sparql-endpoint-connection","title":"Issue: SPARQL Endpoint Connection","text":"<p>If SPARQL queries fail:</p> <ol> <li> <p>Check if the SPARQL endpoint is accessible.</p> </li> <li> <p>Verify that the <code>KG_ENDPOINT_URL</code> variable in your <code>.env</code> file is correctly set.</p> </li> <li> <p>Ensure proper network access/firewall settings.</p> </li> </ol>"},{"location":"getting-started/installation/#mass-spectrometry-data","title":"Mass Spectrometry Data \ud83d\udd2c","text":"<p>By default, \ud83e\uddea MetaboT \ud83c\udf75 connects to the public ENPKG endpoint which hosts an open, annotated mass spectrometry dataset derived from a chemodiverse collection of 1,600 plant extracts. This default dataset enables you to explore all features of \ud83e\uddea MetaboT \ud83c\udf75 without the need for custom data conversion immediately. To use \ud83e\uddea MetaboT \ud83c\udf75 on your mass spectrometry data, the processed and annotated results must first be converted into a knowledge graph format using the ENPKG tool. For more details on converting your own data, please refer to the Experimental Natural Products Knowledge Graph library and the associated publication.</p> <p>Set your SPARQL endpoint by configuring the <code>KG_ENDPOINT_URL</code> variable in your <code>.env</code> file. If you are deploying a local endpoint that requires authentication, add the following variables to your <code>.env</code> file:</p> <pre><code>SPARQL_USERNAME=your_username\nSPARQL_PASSWORD=your_password\n</code></pre> <p>Additionally, to ensure the SPARQL queries generated accurately reflect the schema of your knowledge graph, you must provide detailed information about your knowledge graph\u2019s structure and update the prompt settings in:</p> <ul> <li><code>app/core/agents/validator/prompt.py</code></li> <li>The SPARQL generation chain in <code>app/core/agents/sparql/tool_sparql.py</code></li> </ul>"},{"location":"getting-started/installation/#support","title":"Support \ud83d\udee0\ufe0f","text":"<p>If you encounter any issues during installation:</p> <ol> <li>Check our GitHub Issues for similar problems.</li> <li>Create a new issue with detailed information about your setup and the error.</li> </ol> <p>Next Steps</p> <ul> <li>Follow the Quick Start Guide to begin using \ud83e\uddea MetaboT \ud83c\udf75.</li> <li>Review the Configuration Guide for detailed setup options.</li> <li>Check out Example Usage for practical applications.</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide \ud83d\ude80","text":"<p>Welcome to the Quick Start Guide for \ud83e\uddea MetaboT \ud83c\udf75. This guide will help you quickly run and test the application.</p> <p>\ud83d\udc49 Try the MetaboT Web App Demo: https://metabot.holobiomicslab.eu \u2014 no installation needed!  </p> <p>The demo provides access to an open dataset of 1,600 plant extracts. You can explore metabolomics data and ask questions about the dataset directly through the web interface.</p>"},{"location":"getting-started/quickstart/#prerequisites-for-local-installation","title":"Prerequisites \u2705 (For Local Installation)","text":"<p>Before you begin, ensure that you have:</p> <ul> <li> <p>Completed the Installation Guide</p> </li> <li> <p>Set the necessary environment variables in your <code>.env</code> file:</p> <ul> <li> <p>API key for your chosen language model:</p> <ul> <li><code>OPENAI_API_KEY</code> if using OpenAI</li> <li><code>DEEPSEEK_API_KEY</code> if using DeepSeek</li> <li><code>CLAUDE_API_KEY</code> if using Claude</li> </ul> </li> <li> <p>SPARQL endpoint configuration:</p> <ul> <li><code>KG_ENDPOINT_URL</code> (required)</li> <li><code>SPARQL_USERNAME</code> and <code>SPARQL_PASSWORD</code> (if your endpoint requires authentication)</li> </ul> </li> </ul> </li> <li> <p>Activated your Python virtual environment</p> </li> </ul>"},{"location":"getting-started/quickstart/#running-a-standard-query","title":"Running a Standard Query \ud83d\udd0d","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 includes several predefined queries that demonstrate its capabilities. Those questions could be found here. For example, to run the first standard query (which counts features with matching SIRIUS/CSI:FingerID and ISDB annotations), execute:</p> <pre><code>python -m app.core.main -q 1\n</code></pre> <p>What this does: - Loads the default configuration and dataset - Executes the first query from a list of standard queries - Returns insights based on RDF graph analysis</p>"},{"location":"getting-started/quickstart/#running-a-custom-query","title":"Running a Custom Query \ud83d\udee0\ufe0f","text":"<p>You can also run custom queries tailored to your research needs. For example, to query SIRIUS structural annotations for a specific plant: <pre><code>python -m app.core.main -c \"What are the SIRIUS structural annotations for Tabernaemontana coffeoides?\"\n</code></pre> Note: - Replace the query text in quotes with your desired question. - Ensure that the query is relevant to the metabolomics data available in your configuration.</p>"},{"location":"getting-started/quickstart/#running-in-docker","title":"Running in Docker \ud83d\udc33","text":"<p>If you prefer to run \ud83e\uddea MetaboT within a Docker container, follow these steps:</p> <ol> <li> <p>Build the Docker Image:   Ensure Docker and docker-compose are installed, then run:   <pre><code>docker-compose build\n</code></pre></p> </li> <li> <p>Run the Application in Docker:   To execute the first standard query, run:   <pre><code>docker-compose run metabot python -m app.core.main -q 1\n</code></pre>   This command starts the container and runs the application accordingly. You can adjust the command as needed.</p> </li> </ol>"},{"location":"getting-started/quickstart/#workflow-overview","title":"Workflow Overview \ud83d\udd04","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 leverages a multi-agent workflow architecture to process queries efficiently:</p> <ul> <li>Entry Agent: Processes the incoming query and routes it to the appropriate system.</li> <li>Validator Agent: Immediately verifies that the incoming query is pertinent to the knowledge graph, ensuring its alignment with domain-specific schema.</li> <li>Supervisor Agent: Oversees and coordinates all processing steps within the workflow.</li> <li>ENPKG Agent: Handles domain-specific data processing related to metabolomics.</li> <li>SPARQL Agent: Generates and executes queries against the RDF knowledge graph.</li> <li>Interpreter Agent: Interprets and formats the query results for user readability.</li> </ul> <p>This modular design allows \ud83e\uddea MetaboT \ud83c\udf75 to be extended and customized for various research scenarios.</p>"},{"location":"getting-started/quickstart/#example-scenarios","title":"Example Scenarios \ud83d\udcda","text":""},{"location":"getting-started/quickstart/#basic-feature-analysis","title":"Basic Feature Analysis","text":"<p>Run a standard query to count LCMS features detected in negative ionization mode: <pre><code>python -m app.core.main -c \"Count the number of LCMS features in negative ionization mode\"\n</code></pre></p>"},{"location":"getting-started/quickstart/#chemical-structure-analysis","title":"Chemical Structure Analysis","text":"<p>Obtain structural annotations for a plant sample: <pre><code>python -m app.core.main -c \"What are the SIRIUS structural annotations for Tabernaemontana coffeoides?\"\n</code></pre></p>"},{"location":"getting-started/quickstart/#bioassay-results-exploration","title":"Bioassay Results Exploration","text":"<p>Examine bioassay data for compounds in a specified extract: <pre><code>python -m app.core.main -c \"List the bioassay results at 10\u00b5g/mL against T.cruzi for lab extracts of Tabernaemontana coffeoides\"\n</code></pre></p>"},{"location":"getting-started/quickstart/#interacting-with-the-knowledge-graph","title":"Interacting with the Knowledge Graph \ud83c\udf10","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 connects to a knowledge graph to enrich analysis: <pre><code>import os\nfrom app.core.graph_management.RdfGraphCustom import RdfGraph\n\n# Connect to the knowledge graph using the defined endpoint\n# If SPARQL_USERNAME and SPARQL_PASSWORD environment variables are set,\n# they will be automatically used for authentication\ngraph = RdfGraph(\n    query_endpoint=\"https://enpkg.commons-lab.org/graphdb/repositories/ENPKG\",\n    standard=\"rdf\",\n    auth=None  # Will automatically use environment variables if available\n)\n\n# Or explicitly provide authentication:\nauth = (os.getenv(\"SPARQL_USERNAME\"), os.getenv(\"SPARQL_PASSWORD\"))\ngraph = RdfGraph(\n    query_endpoint=\"your_endpoint_url\",\n    standard=\"rdf\",\n    auth=auth\n)\n</code></pre> Make sure that your <code>KG_ENDPOINT_URL</code> environment variable is correctly set to point to your graph database.</p>"},{"location":"getting-started/quickstart/#advanced-configuration","title":"Advanced Configuration \u2699\ufe0f","text":""},{"location":"getting-started/quickstart/#langsmith-integration","title":"LangSmith Integration","text":"<p>For enhanced tracking and monitoring of workflow runs, we are using LangSmith. An API key is needed (free upon registration) and set the .env variable as follow:</p> <ol> <li>Set up LangSmith:     <pre><code>export LANGCHAIN_API_KEY=\"your_api_key_here\"\nexport LANGCHAIN_PROJECT=\"MetaboT\"\n</code></pre></li> <li>Review LangSmith logs to access runtime details for debugging and auditing.</li> </ol>"},{"location":"getting-started/quickstart/#custom-model-settings","title":"Custom Model Settings","text":"<p>Review and adjust the language model configurations in <code>app/config/params.ini</code>: <pre><code>[llm]\ntemperature=0.0\nid=gpt-4\nmax_retries=3\n</code></pre> This ensures that the models used in your workflows are fine-tuned for your specific analysis needs.</p>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting \ud83d\udc1e","text":"<p>If you encounter issues, consider the following steps:</p> <ul> <li>Environment Variables: Verify that your chosen LLM API key (<code>OPENAI_API_KEY</code>, <code>DEEPSEEK_API_KEY</code>, <code>CLAUDE_API_KEY</code>, etc.) and <code>KG_ENDPOINT_URL</code> are correctly set in your <code>.env</code> file.</li> <li>Knowledge Graph Access: Confirm that the knowledge graph endpoint is reachable and correctly configured.</li> <li>Logs: Review terminal output for any error messages or warnings during execution.</li> </ul> <p>Next Steps \u27a1\ufe0f</p> <ul> <li>Explore the User Guide for in-depth explanations of \ud83e\uddea MetaboT \ud83c\udf75's components.</li> <li>Review the API Reference to understand function details.</li> <li>Examine the Examples for more advanced usage scenarios.</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration Guide \ud83d\udee0\ufe0f","text":"<p>This guide details all configuration options available in \ud83e\uddea MetaboT \ud83c\udf75, helping you customize the system to your specific needs.</p>"},{"location":"user-guide/configuration/#configuration-overview","title":"Configuration Overview \ud83d\udcc1","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 uses several configuration files located in the <code>app/config/</code> directory:</p> <ul> <li><code>params.ini</code>: Language model configurations</li> <li><code>sparql.ini</code>: SPARQL query templates and settings</li> <li><code>logging.ini</code>: Logging configuration</li> <li>.env: Environment variables (created by user)</li> </ul>"},{"location":"user-guide/configuration/#llms-configuration","title":"LLMs Configuration \ud83e\udd16","text":"<p>Located in <code>params.ini</code>, this configuration controls the behavior of different language models used in the system.</p> <pre><code>[llm_preview]\nid = gpt-4-0125-preview\ntemperature = 0\nmax_retries = 3\n\n[llm_o]\nid = gpt-4o\ntemperature = 0\nmax_retries = 3\n\n\n[llm_mini]\nid = gpt-4o-mini\ntemperature = 0\nmax_retries = 3\n\n[llm_o3_mini]\nid = o3-mini-2025-01-31\ntemperature = 1\nmax_retries = 3\n\n[llm_o1]\nid = o1-2024-12-17\ntemperature = 1\nmax_retries = 3\n\n[deepseek_deepseek-chat]\nid = deepseek-chat\ntemperature = 0\nmax_retries = 3\nbase_url = https://api.deepseek.com\n\n[deepseek_deepseek-reasoner]\nid = deepseek-reasoner\ntemperature = 0\nmax_retries = 3\nbase_url = https://api.deepseek.com\n\n[ovh_Meta-Llama-3_1-70B-Instruct]\nid = Meta-Llama-3_1-70B-Instruct\ntemperature = 0\nmax_retries = 3\nbase_url = https://llama-3-1-70b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\n</code></pre>"},{"location":"user-guide/configuration/#available-sections","title":"Available Sections","text":"<ul> <li>Main LLM (<code>[llm_o]</code>)   Production-optimized GPT-4o 2024.08.06 used by most agents.</li> <li>Preview Model (<code>[llm_preview]</code>)   Latest model versions with cutting-edge capabilities.</li> <li>Mini Model (<code>[llm_mini]</code>)   Lightweight GPT-4o variant for basic tasks.</li> <li>O3 Mini (<code>[llm_o3_mini]</code>)   Specialized model for experimental tasks with higher creativity.</li> <li> <p>O1 Core (<code>[llm_o1]</code>)   Advanced model for research and development.</p> </li> <li> <p>DeepSeek Chat (<code>[deepseek_deepseek-chat]</code>)   Conversational model from DeepSeek for interactive queries.</p> </li> <li>DeepSeek Reasoner (<code>[deepseek_deepseek-reasoner]</code>)   Analytical model from DeepSeek for enhanced reasoning.</li> <li>OVH Meta-Llama (<code>[ovh_Meta-Llama-3_1-70B-Instruct]</code>)   Instructive model providing robust language understanding.</li> </ul> <p>\ud83e\uddea MetaboT \ud83c\udf75 supports both OpenAI-compatible API endpoints and various other LLM providers through LiteLLM integration. Models can be configured in two ways:</p> <ol> <li>OpenAI-compatible endpoints using sections like <code>[llm_o]</code>, <code>[deepseek_deepseek-chat]</code>, etc.</li> <li>Other providers through LiteLLM using sections starting with <code>[llm_litellm_]</code>, for example: <pre><code>[llm_litellm_openai]\nid = gpt-4\ntemperature = 0\n\n[llm_litellm_deepseek]\nid = deepseek/deepseek-chat\n\n[llm_litellm_claude]\nid = claude-3-opus-20240229\n\n[llm_litellm_gemini]\nid = gemini/gemini-1.5-pro\n</code></pre></li> </ol> <p>For a complete list of supported providers and their model identifiers, see the LiteLLM Providers Documentation.</p> <p>You can configure different models for each agent in your workflow. For detailed instructions on agent-specific model configuration, refer to the Language Model Configuration Guide.</p>"},{"location":"user-guide/configuration/#parameters","title":"Parameters","text":"<ul> <li><code>id</code>: Model identifier (format depends on provider)</li> <li>For OpenAI-compatible: e.g., gpt-4o, gpt-3.5-turbo</li> <li>For LiteLLM: provider/model-name (e.g., deepseek/deepseek-chat)</li> <li><code>temperature</code>: Randomness in responses (0-1)</li> <li><code>max_retries</code>: Number of retry attempts (optional)</li> <li><code>base_url</code>: Custom API endpoint URL (optional)</li> </ul>"},{"location":"user-guide/configuration/#sparql-configuration","title":"SPARQL Configuration \ud83d\udd0d","text":"<p>The <code>sparql.ini</code> file contains SPARQL query templates and settings essential for interacting with the knowledge graph. These configurations are used by the <code>RdfGraph</code> class to dynamically retrieve the schema from the knowledge graph when no local schema file is provided.</p>"},{"location":"user-guide/configuration/#query-templates","title":"Query Templates","text":"<p><pre><code>[sparqlQueries]\n# Class information query\nCLS_RDF = SELECT DISTINCT ?cls ?com ?label\n        WHERE {\n            ?cls a rdfs:Class .\n            OPTIONAL { ?cls rdfs:comment ?com }\n            OPTIONAL { ?cls rdfs:label ?label }\n        }\n        GROUP BY ?cls ?com ?label\n</code></pre> This query retrieves all classes along with their optional comments and labels. The results form the foundation for constructing the dynamic schema.</p> <p><pre><code># Class relationships query\nCLS_REL_RDF = SELECT ?property (SAMPLE(COALESCE(?type, STR(DATATYPE(?value)), \"Untyped\")) AS ?valueType) \n        WHERE {...}\n</code></pre> This query is executed for each class retrieved by <code>CLS_RDF</code>. It retrieves properties associated with instances of the specified class and determines a representative value type for each property.</p>"},{"location":"user-guide/configuration/#excluded-uris","title":"Excluded URIs","text":"<p><pre><code>[excludedURIs]\nuris = http://www.w3.org/1999/02/22-rdf-syntax-ns#type,\n       http://www.w3.org/2000/01/rdf-schema#comment,\n       http://www.w3.org/2000/01/rdf-schema#Class,\n       http://xmlns.com/foaf/0.1/depiction\n</code></pre> The list of excluded URIs defines properties that are filtered out during the schema retrieval process.</p>"},{"location":"user-guide/configuration/#logging-configuration","title":"Logging Configuration \ud83d\udcdd","text":"<p>The <code>logging.ini</code> file controls the logging behavior of \ud83e\uddea MetaboT \ud83c\udf75.</p>"},{"location":"user-guide/configuration/#logger-settings","title":"Logger Settings","text":"<pre><code>[loggers]\nkeys=root\n\n[logger_root]\nlevel=INFO\nhandlers=consoleHandler,fileHandler\n</code></pre>"},{"location":"user-guide/configuration/#handler-configuration","title":"Handler Configuration","text":"<ul> <li>Console Handler <pre><code>[handler_consoleHandler]\nclass=StreamHandler\nlevel=INFO\nformatter=simpleFormatter\nargs=(sys.stdout,)\n</code></pre></li> <li>File Handler <pre><code>[handler_fileHandler]\nclass=logging.handlers.RotatingFileHandler\nlevel=INFO\nformatter=detailedFormatter\nargs=('./app/config/logs/app.log', 'w', 100000, 5)\n</code></pre></li> </ul>"},{"location":"user-guide/configuration/#formatter-settings","title":"Formatter Settings","text":"<ul> <li>Simple Formatter (Console) <pre><code>[formatter_simpleFormatter]\nformat=%(name)s - %(levelname)s - %(message)s\n</code></pre></li> <li>Detailed Formatter (File) <pre><code>[formatter_detailedFormatter]\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\ndatefmt=%Y-%m-%d %H:%M:%S\n</code></pre></li> </ul>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables \ud83d\udd27","text":"<p>Create a <code>.env</code> file in the project root with these variables:</p> <pre><code># LLM API Configuration\nOPENAI_API_KEY=your_openai_api_key          # If using OpenAI models\nDEEPSEEK_API_KEY=your_deepseek_api_key      # If using DeepSeek models\nCLAUDE_API_KEY=your_claude_api_key          # If using Claude models\nGEMINI_API_KEY=your_gemini_api_key          # If using Gemini models\nOVHCLOUD_API_KEY=your_ovh_api_key           # If using Llama on OVHcloud\n\n# Knowledge Graph Configuration\nKG_ENDPOINT_URL=https://enpkg.commons-lab.org/graphdb/repositories/ENPKG\nSPARQL_USERNAME=your_username               # If endpoint requires authentication\nSPARQL_PASSWORD=your_password               # If endpoint requires authentication\n\n# LangSmith Configuration (Optional)\nLANGCHAIN_API_KEY=your_langsmith_api_key\nLANGCHAIN_PROJECT=MetaboT\nLANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n</code></pre>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices \ud83d\udcd8","text":""},{"location":"user-guide/configuration/#language-model-selection","title":"Language Model Selection","text":"<ul> <li>Use  <code>llm_o</code> for complex queries requiring high accuracy.</li> <li>Use  <code>llm_o3_mini</code> for faster, cost-effective operations.</li> <li>Consider <code>llm_o3</code> or <code>llm_o1</code>and its variants for complex questions.</li> </ul>"},{"location":"user-guide/configuration/#logging-configuration_1","title":"Logging Configuration","text":"<ul> <li>Keep the default INFO level for production.</li> <li>Use the DEBUG level during development.</li> <li>Monitor log file sizes (default 1MB per file, 5 files max).</li> </ul>"},{"location":"user-guide/configuration/#sparql-optimization","title":"SPARQL Optimization","text":"<ul> <li>Review and update excluded URIs as needed.</li> <li>Monitor query performance.</li> <li>Adjust the prompt of sparql_generation_select_chain in <code>app/core/agents/sparql/tool_sparql</code>.</li> </ul>"},{"location":"user-guide/configuration/#environment-security","title":"Environment Security","text":"<ul> <li>Never commit the <code>.env</code> file to version control.</li> <li>Rotate API keys regularly.</li> <li>Use different keys for development and production.</li> </ul>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting \ud83d\udea8","text":""},{"location":"user-guide/configuration/#common-issues","title":"Common Issues","text":"<ul> <li> <p>Language Model Errors </p> <ul> <li>Check API key validity.</li> <li>Verify model availability.</li> <li>Review rate limits.</li> </ul> </li> <li> <p>Logging Issues </p> <ul> <li>Ensure write permissions for the log directory.</li> <li>Check disk space.</li> <li>Verify log rotation settings.</li> </ul> </li> <li> <p>SPARQL Problems </p> <ul> <li>Validate endpoint accessibility.</li> <li>Check query syntax.</li> <li>Review timeout settings.</li> </ul> </li> </ul>"},{"location":"user-guide/configuration/#default-dataset-and-data-conversion","title":"Default Dataset and Data Conversion \ud83d\udcca","text":"<p>Note:  By default, \ud83e\uddea MetaboT \ud83c\udf75 connects to the public ENPKG endpoint which hosts an open, annotated mass spectrometry dataset derived from a chemodiverse collection of 1,600 plant extracts. This default dataset enables you to explore all features of \ud83e\uddea MetaboT \ud83c\udf75 without the need for custom data conversion immediately. To use \ud83e\uddea MetaboT \ud83c\udf75 on your mass spectrometry data, the processed and annotated results must first be converted into a knowledge graph format using the ENPKG tool. </p> <ul> <li>Update the <code>KG_ENDPOINT_URL</code> in your .env file to point to your custom knowledge graph endpoint.</li> </ul>"},{"location":"user-guide/overview/","title":"\ud83e\uddea MetaboT \ud83c\udf75 Overview \u2728","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 is an advanced metabolomics analysis tool that combines AI-powered agents, graph-based data management, and sophisticated query capabilities to analyze and interpret metabolomics data.</p>"},{"location":"user-guide/overview/#system-architecture","title":"System Architecture \ud83c\udfd7\ufe0f","text":""},{"location":"user-guide/overview/#core-components","title":"Core Components \u2699\ufe0f","text":"<pre><code>graph TB\n    A[User Query] --&gt; B[Entry Agent]\n    B --&gt; G[Validator Agent]\n    G --&gt; C[Supervisor Agent]\n    C &lt;--&gt; D[ENPKG Agent]\n    C &lt;--&gt; E[SPARQL Agent]\n    C &lt;--&gt; F[Interpreter Agent]\n     E  --&gt; H[Knowledge Graph]\n\n</code></pre> <ul> <li> <p>Entry Agent \ud83d\udeaa</p> <ul> <li>Accepts user queries and input files (if provided) and performs initial processing.</li> </ul> </li> <li> <p>Validator Agent \u2705</p> <ul> <li>Validates user questions for knowledge graph.</li> <li>Verifies plant names using the database.</li> <li>Checks question content against the knowledge graph schema.</li> </ul> </li> <li> <p>Supervisor Agent \ud83c\udf9b\ufe0f</p> <ul> <li>Orchestrates the workflow between agents.</li> <li>Manages state and context throughout query processing.</li> <li>Ensures proper sequencing of operations.</li> </ul> </li> <li> <p>ENPKG Agent \ud83e\uddea</p> <ul> <li>Handles metabolomics-specific processing.</li> <li>Provides resolutions to the entities mentioned in the question.</li> </ul> </li> <li> <p>SPARQL Agent \ud83d\udd0e</p> <ul> <li>Generates and executes queries against the RDF knowledge graph.</li> <li>Optimizes query performance.</li> <li>Handles complex graph traversals.</li> </ul> </li> <li> <p>Interpreter Agent \ud83d\udce2</p> <ul> <li>Processes and formats query results.</li> <li>Generates human-readable outputs.</li> <li>Handles data visualization requests.</li> </ul> </li> </ul>"},{"location":"user-guide/overview/#knowledge-graph-integration","title":"Knowledge Graph Integration \ud83d\udd17","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 utilizes a sophisticated RDF-based knowledge graph that:</p> <ul> <li>Stores metabolomics data and relationships.</li> <li>Enables complex query capabilities.</li> <li>Supports data integration from multiple sources.</li> <li>Maintains data provenance.</li> </ul>"},{"location":"user-guide/overview/#key-features","title":"Key Features \ud83d\ude80","text":""},{"location":"user-guide/overview/#query-processing","title":"Query Processing \ud83d\udd0d","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 supports various types of queries:</p> <ul> <li>Standard Queries: Pre-defined queries for common analyses.</li> <li>Custom Queries: User-defined natural language queries.</li> <li>Knowledge Graph Integration: Access and analyze data from a comprehensive knowledge graph.</li> <li>Visualization Tools: Generate visualizations to better understand your data.</li> </ul> <p>For development updates, please refer to the <code>dev</code> branch.</p>"},{"location":"user-guide/overview/#ai-powered-processing","title":"AI-Powered Processing \ud83e\udd16","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 leverages advanced AI capabilities through:</p> <ul> <li> <p>Language Models</p> <ul> <li>Natural language query processing</li> <li>Context-aware responses</li> <li>Result interpretation</li> </ul> </li> <li> <p>Agent Collaboration</p> <ul> <li>Multi-agent workflow coordination</li> <li>Specialized task processing</li> <li>Adaptive response generation</li> </ul> </li> </ul>"},{"location":"user-guide/overview/#workflow-examples","title":"Workflow Examples \ud83d\udee0\ufe0f","text":""},{"location":"user-guide/overview/#basic-feature-analysis","title":"Basic Feature Analysis \ud83d\udcdd","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Entry\n    participant Validator\n    participant Supervisor\n    participant ENPKG\n    participant SPARQL \n    participant Graph\n    participant Interpreter\n\n    User-&gt;&gt;Entry: Submit feature query\n    Entry-&gt;&gt;Validator: Preprocess the query\n    Validator-&gt;&gt;Supervisor: Validate the question\n    Supervisor-&gt;&gt;ENPKG:Select the next agent \n    Supervisor-&gt;&gt;SPARQL: Provide the question and resolved entities\n    Supervisor-&gt;&gt;Interpreter: Provide the results\n    SPARQL-&gt;&gt;Graph: Generate and execute SPARQL query \n    ENPKG--&gt;&gt;Supervisor: Provide resolved entities\n    SPARQL--&gt;&gt;Supervisor: Provide the results\n    Interpreter--&gt;&gt;Supervisor: Provide the interpreted results\n    Supervisor--&gt;&gt;User: Present final results</code></pre>"},{"location":"user-guide/overview/#performances","title":"Performances  \u26a1\ufe0f","text":""},{"location":"user-guide/overview/#query-optimization","title":"Query Optimization \ud83d\udd27","text":"<ul> <li>Use highly targeted, knowledge-graph-centric queries that are clearly formatted</li> <li>Leverage standard queries for common operations</li> <li>Consider query complexity and data volume</li> </ul>"},{"location":"user-guide/overview/#best-practices","title":"Best Practices \ud83d\udc4d","text":"<ol> <li> <p>Query Design</p> <ul> <li>Start with standard queries when possible</li> <li>Build custom queries incrementally</li> <li>Test queries with smaller datasets first</li> </ul> </li> <li> <p>System Configuration</p> <ul> <li>Keep environment variables updated</li> <li>Monitor system resources</li> <li>Regular maintenance of graph database</li> </ul> </li> </ol>"},{"location":"user-guide/overview/#integration-capabilities","title":"Integration Capabilities \ud83d\udd0c","text":"<p>\ud83e\uddea MetaboT \ud83c\udf75 can be integrated with:</p> <ul> <li>External databases</li> <li>Custom analysis pipelines</li> <li>Visualization tools</li> <li>Reporting systems</li> </ul>"},{"location":"user-guide/overview/#future-developments","title":"Future Developments \ud83d\udd2e","text":"<p>Planned enhancements include:</p> <ul> <li>Enhanced visualization capabilities</li> <li>Additional analysis algorithms</li> <li>Expanded database integrations</li> <li>Improved performance optimization</li> </ul> <p>For detailed information about specific components, please refer to the respective sections in the documentation.</p>"}]}