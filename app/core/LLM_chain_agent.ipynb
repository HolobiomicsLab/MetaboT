{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client (API URL: https://api.smith.langchain.com)\n"
     ]
    }
   ],
   "source": [
    "#remove\n",
    "# #Setting up the LangSmith\n",
    "# #For now, all runs will be stored in the \"KGBot Testing - GPT4\"\n",
    "# #If you want to separate the traces to have a better control of specific traces.\n",
    "# #Metadata as llm version and temperature can be obtaneid from traces. \n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"KGBot Testing - GPT4\" #Please update the name here if you want to create a new project for separating the traces. \n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "# #Check if the client was initialized\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain imports for agent and prompt handling\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser \n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "# langgraph imports for prebuilt tool invocation\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# langchain_core imports for message handling and action schema\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, FunctionMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "\n",
    "# langchain output parser for OpenAI functions\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "# langchain_core prompts for chat template and message placeholders\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# typing imports for type hinting\n",
    "from typing import Annotated, List, Tuple, Union,  Any, Dict, Optional, Sequence, TypedDict\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "\n",
    "# Standard library imports for JSON and regular expressions\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Custom imports for RDF graph manipulation and chemical, target, taxon, and SPARQL resolution\n",
    "from RdfGraphCustom import RdfGraph\n",
    "from smile_resolver import smiles_to_inchikey\n",
    "from chemical_resolver import ChemicalResolver\n",
    "from target_resolver import target_name_to_target_id\n",
    "from taxon_resolver import TaxonResolver\n",
    "from sparql import GraphSparqlQAChain\n",
    "\n",
    "# langchain pydantic for base model definitions\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# langchain tools for base, structured tool definitions, and tool decorators\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "# Standard library import for object serialization\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:49:54,133 - INFO - query \n",
      "    SELECT DISTINCT ?cls ?com ?label\n",
      "        WHERE {\n",
      "            ?cls a rdfs:Class . \n",
      "            OPTIONAL { ?cls rdfs:comment ?com }\n",
      "            OPTIONAL { ?cls rdfs:label ?label }\n",
      "        }\n",
      "        GROUP BY ?cls ?com ?label\n",
      "    \n",
      "Adding classes to graph: 100%|██████████| 50/50 [00:09<00:00,  5.44it/s]\n",
      "2024-02-26 15:50:03,424 - INFO - namespaces [('brick', 'https://brickschema.org/schema/Brick#'), ('csvw', 'http://www.w3.org/ns/csvw#'), ('dc', 'http://purl.org/dc/elements/1.1/'), ('dcat', 'http://www.w3.org/ns/dcat#'), ('dcmitype', 'http://purl.org/dc/dcmitype/'), ('dcterms', 'http://purl.org/dc/terms/'), ('dcam', 'http://purl.org/dc/dcam/'), ('doap', 'http://usefulinc.com/ns/doap#'), ('foaf', 'http://xmlns.com/foaf/0.1/'), ('geo', 'http://www.opengis.net/ont/geosparql#'), ('odrl', 'http://www.w3.org/ns/odrl/2/'), ('org', 'http://www.w3.org/ns/org#'), ('prof', 'http://www.w3.org/ns/dx/prof/'), ('prov', 'http://www.w3.org/ns/prov#'), ('qb', 'http://purl.org/linked-data/cube#'), ('schema', 'https://schema.org/'), ('sh', 'http://www.w3.org/ns/shacl#'), ('skos', 'http://www.w3.org/2004/02/skos/core#'), ('sosa', 'http://www.w3.org/ns/sosa/'), ('ssn', 'http://www.w3.org/ns/ssn/'), ('time', 'http://www.w3.org/2006/time#'), ('vann', 'http://purl.org/vocab/vann/'), ('void', 'http://rdfs.org/ns/void#'), ('wgs', 'https://www.w3.org/2003/01/geo/wgs84_pos#'), ('owl', 'http://www.w3.org/2002/07/owl#'), ('rdf', 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'), ('rdfs', 'http://www.w3.org/2000/01/rdf-schema#'), ('xsd', 'http://www.w3.org/2001/XMLSchema#'), ('xml', 'http://www.w3.org/XML/1998/namespace'), ('ns1', 'https://enpkg.commons-lab.org/kg/'), ('ns2', 'https://enpkg.commons-lab.org/module/')]\n",
      "2024-02-26 15:50:03,430 - INFO - number of tokens 5893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The namespace prefixes are: [('brick', 'https://brickschema.org/schema/Brick#'), ('csvw', 'http://www.w3.org/ns/csvw#'), ('dc', 'http://purl.org/dc/elements/1.1/'), ('dcat', 'http://www.w3.org/ns/dcat#'), ('dcmitype', 'http://purl.org/dc/dcmitype/'), ('dcterms', 'http://purl.org/dc/terms/'), ('dcam', 'http://purl.org/dc/dcam/'), ('doap', 'http://usefulinc.com/ns/doap#'), ('foaf', 'http://xmlns.com/foaf/0.1/'), ('geo', 'http://www.opengis.net/ont/geosparql#'), ('odrl', 'http://www.w3.org/ns/odrl/2/'), ('org', 'http://www.w3.org/ns/org#'), ('prof', 'http://www.w3.org/ns/dx/prof/'), ('prov', 'http://www.w3.org/ns/prov#'), ('qb', 'http://purl.org/linked-data/cube#'), ('schema', 'https://schema.org/'), ('sh', 'http://www.w3.org/ns/shacl#'), ('skos', 'http://www.w3.org/2004/02/skos/core#'), ('sosa', 'http://www.w3.org/ns/sosa/'), ('ssn', 'http://www.w3.org/ns/ssn/'), ('time', 'http://www.w3.org/2006/time#'), ('vann', 'http://purl.org/vocab/vann/'), ('void', 'http://rdfs.org/ns/void#'), ('wgs', 'https://www.w3.org/2003/01/geo/wgs84_pos#'), ('owl', 'http://www.w3.org/2002/07/owl#'), ('rdf', 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'), ('rdfs', 'http://www.w3.org/2000/01/rdf-schema#'), ('xsd', 'http://www.w3.org/2001/XMLSchema#'), ('xml', 'http://www.w3.org/XML/1998/namespace'), ('ns1', 'https://enpkg.commons-lab.org/kg/'), ('ns2', 'https://enpkg.commons-lab.org/module/')]\n",
      "In the following, each URI is followed by the local name and optionally its rdfs:Label, and optionally its rdfs:comment. \n",
      "The RDF graph supports the following node types:\n",
      "<rdf:XMLLiteral> (XMLLiteral, , ), <xsd:nonNegativeInteger> (nonNegativeInteger, , ), <xsd:string> (string, , ), <ns1:RawMaterial> (RawMaterial, A RawMaterial, A raw laboratory biological material, i.e. before extraction), <ns1:LFpair> (LFpair, pair of LCMSFeature, A pair of 2 LCMSFeature), <ns1:WDChemical> (WDChemical, Cross-reference to a chemical entity in Wikidata, Cross-reference to a chemical entity in Wikidata), <ns1:WDTaxon> (WDTaxon, Cross-reference to a taxon in Wikidata, Cross-reference to a taxon in Wikidata), <ns1:InChIkey2D> (InChIkey2D, 2D InChIKey, The first 14 characters of an InChIKey, often returned by MS-based annotation tools), <ns1:InChIkey> (InChIkey, InChIKey, A chemical structure represented by its InChIKey), <ns1:LabExtract> (LabExtract, A LabExtract, A natural extract obtained from the processing of a RawMaterial), <ns1:LCMSAnalysisPos> (LCMSAnalysisPos, Pos LCMS analysis, An LCMS analysis in positive ionization mode (pos)), <ns1:LCMSFeatureList> (LCMSFeatureList, Feature list, A list of LCMS features obtained from the processing of a given LCMS analysis), <ns1:SiriusCanopusAnnotation> (SiriusCanopusAnnotation, CANOPUS chemical class annotation, A spectrum chemical class annotation by SIRIUS-CANOPUS), <ns1:LCMSFeature> (LCMSFeature, LCMS individual MS2 spectrum, An LCMS feature from a processed LCMS analysis), <ns1:Spec2VecLoss> (Spec2VecLoss, A Spec2VecLoss, A Spec2VecLoss that partly characterizes an MS2Spectrum), <ns1:NPCClass> (NPCClass, NPCClass, A NPClassifier (NPC) chemical class), <ns1:NPCSuperclass> (NPCSuperclass, NPCSuperclass, A NPClassifier (NPC) chemical superclass), <ns1:Spec2VecPeak> (Spec2VecPeak, A Spec2VecPeak, A Spec2VecPeak that partly characterizes an MS2 spectrum), <ns1:SiriusStructureAnnotation> (SiriusStructureAnnotation, SIRIUS structural annotation, A spectrum structural annotation by SIRIUS), <ns1:Spec2VecDoc> (Spec2VecDoc, A Spec2VecDoc, An ensemble of Spec2VecPeak and Spec2VecLoss objects that characterizes an MS2Spectrum), <ns1:IsdbAnnotation> (IsdbAnnotation, ISDB structural annotation, A spectrum structural annotation by comparison with an in-silico spectral DB, coupled to chemical and taxonomical reweighting), <ns1:NPCPathway> (NPCPathway, NPCPathway, A NPClassifier (NPC) chemical pathway), <ns1:LCMSAnalysisNeg> (LCMSAnalysisNeg, Neg LCMS analysis, An LCMS analysis in negative ionization mode (neg)), <ns2:L610ugml> (L610ugml, L610ugml, A screening result at 10ug/mL from a phenotypic assay against L6 cells), <ns2:Ldono10ugml> (Ldono10ugml, Ldono10ugml, A screening result at 10ug/mL from a phenotypic assay against L.donovani), <ns2:Ldono2ugml> (Ldono2ugml, Ldono2ugml, A screening result at 2ug/mL from a phenotypic assay against L.donovani), <ns2:Tbrucei10ugml> (Tbrucei10ugml, Tbrucei10ugml, A screening result at 10ug/mL from a phenotypic assay against T.brucei rhodesiense), <ns2:Tbrucei2ugml> (Tbrucei2ugml, Tbrucei2ugml, A screening result at 2ug/mL from a phenotypic assay against T.brucei rhodesiense), <ns2:Tcruzi10ugml> (Tcruzi10ugml, Tcruzi10ugml, A screening result at 10ug/mL from a phenotypic assay against T.cruzi), <ns2:ChEMBLTarget> (ChEMBLTarget, A ChEMBL target, A ChEMBL target), <ns1:LabBlank> (LabBlank, A LabBlank, A blank sample), <ns1:LabQc> (LabQc, A LabQc, A quality control (QC) sample), <ns2:ChEMBLAssayResults> (ChEMBLAssayResults, A ChEMBL assay result, A ChEMBL assay result), <ns2:ChEMBLAssay> (ChEMBLAssay, A ChEMBL assay, A ChEMBL assay), <ns2:ChEMBLChemical> (ChEMBLChemical, A ChEMBL chemical, A ChEMBL chemical), <ns2:ChEMBLDocument> (ChEMBLDocument, A ChEMBL document, A ChEMBL document), <ns1:LabObject> (LabObject, A LabObject, An object that correspond to a physical laboratory object), <ns1:BioAssayResults> (BioAssayResults, A bioassay result, An object to store bioactivity results), <ns1:MS2Spectrum> (MS2Spectrum, MS2 spectrum, A fragmentation mass spectrometry (or MS2) spectrum), <ns1:LCMSAnalysis> (LCMSAnalysis, LCMS analysis, An LCMS analysis in a given ionization mode (pos or neg)), <ns1:Annotation> (Annotation, Spectrum annotation, A spectral annotation), <ns1:GNPSAnnotation> (GNPSAnnotation, GNPS structural annotation, A spectrum structural annotation by GNPS), <ns1:SpectralPair> (SpectralPair, pair of MS2Spectra, A pair of 2 MS2Spectra), <ns1:ChemicalEntity> (ChemicalEntity, chemical entity, ), <ns1:ChemicalEntity> (ChemicalEntity, A chemical entity (chemical structure or class), ), <ns1:ChemicalTaxonomy> (ChemicalTaxonomy, chemical taxonomy, ), <ns1:ChemicalTaxonomy> (ChemicalTaxonomy, A chemical taxonmy (chemical class), ), <ns1:XRef> (XRef, Any cross-reference, Any cross-reference), <ns1:spec2vec> (spec2vec, A spec2vec-related object, A spec2vec-related object), <ns2:SwissTPHBioAssay> (SwissTPHBioAssay, SwissTPHBioAssay, A bioasay result from Swiss Tropical and Public Health Institute (sTPH))\n",
      "The RDF graph have the following schema:\n",
      "@prefix ns1: <https://enpkg.commons-lab.org/kg/> .\n",
      "@prefix ns2: <https://enpkg.commons-lab.org/module/> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "ns1:LFpair ns1:has_cosine xsd:float ;\n",
      "    ns1:has_mass_difference xsd:float ;\n",
      "    ns1:has_member ns1:LCMSFeature,\n",
      "        ns1:MS2Spectrum ;\n",
      "    ns1:has_mn_params [ ] .\n",
      "\n",
      "ns1:LabBlank rdfs:label xsd:string ;\n",
      "    ns1:has_LCMS ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisNeg .\n",
      "\n",
      "ns1:LabQc rdfs:label xsd:string ;\n",
      "    ns1:has_LCMS ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisNeg .\n",
      "\n",
      "ns1:RawMaterial ns1:has_LCMS xsd:string,\n",
      "        ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:has_lab_process ns1:LabExtract,\n",
      "        ns1:LabObject ;\n",
      "    ns1:has_lcms_feature_list xsd:string,\n",
      "        ns1:LabExtract,\n",
      "        ns1:LabObject,\n",
      "        ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:has_sirius_annotation xsd:string,\n",
      "        ns1:LabExtract,\n",
      "        ns1:LabObject,\n",
      "        ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:has_unresolved_taxon [ ] ;\n",
      "    ns1:has_wd_id ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:submitted_taxon xsd:string ;\n",
      "    ns2:has_broad_organe [ ] ;\n",
      "    ns2:has_organe [ ] ;\n",
      "    ns2:has_subsystem [ ] ;\n",
      "    ns2:has_tissue [ ] .\n",
      "\n",
      "ns1:SpectralPair ns1:has_cosine xsd:float ;\n",
      "    ns1:has_mass_difference xsd:float ;\n",
      "    ns1:has_member ns1:LCMSFeature,\n",
      "        ns1:MS2Spectrum ;\n",
      "    ns1:has_mn_params [ ] .\n",
      "\n",
      "ns1:ChemicalEntity ns1:has_npc_class ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_npc_pathway ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_npc_superclass ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_smiles xsd:string ;\n",
      "    ns1:has_wd_id ns1:WDChemical,\n",
      "        ns1:XRef ;\n",
      "    ns2:has_chembl_id ns1:XRef,\n",
      "        ns2:ChEMBLChemical .\n",
      "\n",
      "ns1:InChIkey ns1:has_npc_class ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_npc_pathway ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_npc_superclass ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_smiles xsd:string ;\n",
      "    ns1:has_wd_id ns1:WDChemical,\n",
      "        ns1:XRef ;\n",
      "    ns2:has_chembl_id ns1:XRef,\n",
      "        ns2:ChEMBLChemical .\n",
      "\n",
      "ns2:ChEMBLAssayResults rdfs:label xsd:string ;\n",
      "    ns2:activity_relation xsd:string ;\n",
      "    ns2:activity_type xsd:string ;\n",
      "    ns2:activity_unit xsd:string ;\n",
      "    ns2:activity_value xsd:float ;\n",
      "    ns2:assay_id ns1:XRef,\n",
      "        ns2:ChEMBLAssay ;\n",
      "    ns2:stated_in_document ns1:XRef,\n",
      "        ns2:ChEMBLDocument ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget ;\n",
      "    ns2:target_name xsd:string .\n",
      "\n",
      "ns2:ChEMBLDocument ns2:journal_name xsd:string .\n",
      "\n",
      "ns1:BioAssayResults rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns1:SiriusCanopusAnnotation rdfs:label xsd:string ;\n",
      "    ns1:has_canopus_npc_class ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass ;\n",
      "    ns1:has_canopus_npc_class_prob xsd:float ;\n",
      "    ns1:has_canopus_npc_pathway ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCPathway ;\n",
      "    ns1:has_canopus_npc_pathway_prob xsd:float ;\n",
      "    ns1:has_canopus_npc_superclass ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_canopus_npc_superclass_prob xsd:float .\n",
      "\n",
      "ns1:SiriusStructureAnnotation rdfs:label xsd:string ;\n",
      "    ns1:has_InChIkey2D ns1:InChIkey2D ;\n",
      "    ns1:has_cosmic_score xsd:float ;\n",
      "    ns1:has_ionization xsd:string ;\n",
      "    ns1:has_sirius_adduct xsd:string ;\n",
      "    ns1:has_sirius_score xsd:float ;\n",
      "    ns1:has_zodiac_score xsd:float .\n",
      "\n",
      "ns1:Spec2VecDoc rdfs:label xsd:string ;\n",
      "    ns1:has_spec2vec_loss ns1:Spec2VecLoss,\n",
      "        ns1:spec2vec ;\n",
      "    ns1:has_spec2vec_peak ns1:Spec2VecPeak,\n",
      "        ns1:spec2vec .\n",
      "\n",
      "ns1:Spec2VecLoss rdfs:label xsd:string ;\n",
      "    ns1:has_value xsd:float .\n",
      "\n",
      "ns1:Spec2VecPeak rdfs:label xsd:string ;\n",
      "    ns1:has_value xsd:float .\n",
      "\n",
      "ns2:ChEMBLChemical ns2:has_chembl_activity ns1:XRef,\n",
      "        ns2:ChEMBLAssayResults .\n",
      "\n",
      "ns2:L610ugml rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float .\n",
      "\n",
      "ns2:Ldono10ugml rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns2:Ldono2ugml rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns2:SwissTPHBioAssay rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns2:Tbrucei10ugml rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns2:Tbrucei2ugml rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns2:Tcruzi10ugml rdfs:label xsd:string ;\n",
      "    ns2:inhibition_percentage xsd:float ;\n",
      "    ns2:target_id ns1:XRef,\n",
      "        ns2:ChEMBLTarget .\n",
      "\n",
      "ns1:InChIkey2D ns1:has_npc_class ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_npc_pathway ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_npc_superclass ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass,\n",
      "        ns1:NPCPathway,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_smiles xsd:string ;\n",
      "    ns1:is_InChIkey2D_of ns1:ChemicalEntity,\n",
      "        ns1:InChIkey .\n",
      "\n",
      "ns1:LCMSFeature rdfs:label xsd:string ;\n",
      "    ns1:fast_search_gnpsdata_index_analog [ ] ;\n",
      "    ns1:fast_search_gnpsdata_index_no_analog [ ] ;\n",
      "    ns1:fast_search_gnpslibrary_analog [ ] ;\n",
      "    ns1:fast_search_gnpslibrary_no_analog [ ] ;\n",
      "    ns1:gnps_dashboard_view [ ] ;\n",
      "    ns1:has_canopus_annotation ns1:Annotation,\n",
      "        ns1:SiriusCanopusAnnotation ;\n",
      "    ns1:has_fbmn_ci [ ] ;\n",
      "    ns1:has_feature_area xsd:float ;\n",
      "    ns1:has_ionization xsd:string ;\n",
      "    ns1:has_isdb_annotation ns1:Annotation,\n",
      "        ns1:IsdbAnnotation ;\n",
      "    ns1:has_parent_mass xsd:float ;\n",
      "    ns1:has_raw_spectrum xsd:string ;\n",
      "    ns1:has_relative_feature_area xsd:float ;\n",
      "    ns1:has_retention_time xsd:float ;\n",
      "    ns1:has_row_id xsd:decimal ;\n",
      "    ns1:has_sirius_annotation ns1:Annotation,\n",
      "        ns1:IsdbAnnotation,\n",
      "        ns1:SiriusStructureAnnotation ;\n",
      "    ns1:has_spec2vec_doc ns1:Spec2VecDoc,\n",
      "        ns1:spec2vec ;\n",
      "    ns1:has_usi xsd:string .\n",
      "\n",
      "ns1:MS2Spectrum rdfs:label xsd:string ;\n",
      "    ns1:fast_search_gnpsdata_index_analog [ ] ;\n",
      "    ns1:fast_search_gnpsdata_index_no_analog [ ] ;\n",
      "    ns1:fast_search_gnpslibrary_analog [ ] ;\n",
      "    ns1:fast_search_gnpslibrary_no_analog [ ] ;\n",
      "    ns1:gnps_dashboard_view [ ] ;\n",
      "    ns1:has_canopus_annotation ns1:Annotation,\n",
      "        ns1:SiriusCanopusAnnotation ;\n",
      "    ns1:has_fbmn_ci [ ] ;\n",
      "    ns1:has_feature_area xsd:float ;\n",
      "    ns1:has_ionization xsd:string ;\n",
      "    ns1:has_isdb_annotation ns1:Annotation,\n",
      "        ns1:IsdbAnnotation ;\n",
      "    ns1:has_parent_mass xsd:float ;\n",
      "    ns1:has_raw_spectrum xsd:string ;\n",
      "    ns1:has_relative_feature_area xsd:float ;\n",
      "    ns1:has_retention_time xsd:float ;\n",
      "    ns1:has_row_id xsd:decimal ;\n",
      "    ns1:has_sirius_annotation ns1:Annotation,\n",
      "        ns1:IsdbAnnotation,\n",
      "        ns1:SiriusStructureAnnotation ;\n",
      "    ns1:has_spec2vec_doc ns1:Spec2VecDoc,\n",
      "        ns1:spec2vec ;\n",
      "    ns1:has_usi xsd:string .\n",
      "\n",
      "ns1:IsdbAnnotation rdfs:label xsd:string ;\n",
      "    ns1:has_InChIkey2D ns1:InChIkey2D ;\n",
      "    ns1:has_adduct xsd:string ;\n",
      "    ns1:has_consistency_score xsd:float ;\n",
      "    ns1:has_final_score xsd:float ;\n",
      "    ns1:has_spectral_score xsd:float ;\n",
      "    ns1:has_taxo_score xsd:float .\n",
      "\n",
      "ns1:LCMSAnalysisNeg ns1:has_gnpslcms_link [ ] ;\n",
      "    ns1:has_lcms_feature_list ns1:LCMSFeatureList ;\n",
      "    ns1:has_massive_doi [ ] ;\n",
      "    ns1:has_massive_license [ ] .\n",
      "\n",
      "ns1:LCMSFeatureList ns1:has_ionization xsd:string ;\n",
      "    ns1:has_lcms_feature ns1:LCMSFeature,\n",
      "        ns1:MS2Spectrum .\n",
      "\n",
      "ns1:Annotation rdfs:label xsd:string ;\n",
      "    ns1:has_InChIkey2D ns1:InChIkey2D ;\n",
      "    ns1:has_adduct xsd:string ;\n",
      "    ns1:has_canopus_npc_class ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCClass ;\n",
      "    ns1:has_canopus_npc_class_prob xsd:float ;\n",
      "    ns1:has_canopus_npc_pathway ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCPathway ;\n",
      "    ns1:has_canopus_npc_pathway_prob xsd:float ;\n",
      "    ns1:has_canopus_npc_superclass ns1:ChemicalTaxonomy,\n",
      "        ns1:NPCSuperclass ;\n",
      "    ns1:has_canopus_npc_superclass_prob xsd:float ;\n",
      "    ns1:has_consistency_score xsd:float ;\n",
      "    ns1:has_cosmic_score xsd:float ;\n",
      "    ns1:has_final_score xsd:float ;\n",
      "    ns1:has_ionization xsd:string ;\n",
      "    ns1:has_sirius_adduct xsd:string ;\n",
      "    ns1:has_sirius_score xsd:float ;\n",
      "    ns1:has_spectral_score xsd:float ;\n",
      "    ns1:has_taxo_score xsd:float ;\n",
      "    ns1:has_zodiac_score xsd:float .\n",
      "\n",
      "ns1:LCMSAnalysisPos ns1:has_gnpslcms_link [ ] ;\n",
      "    ns1:has_lcms_feature_list ns1:LCMSFeatureList ;\n",
      "    ns1:has_massive_doi [ ] ;\n",
      "    ns1:has_massive_license [ ] ;\n",
      "    ns1:has_sirius_annotation ns1:LCMSFeatureList .\n",
      "\n",
      "ns1:LabExtract rdfs:label xsd:string ;\n",
      "    ns1:has_LCMS ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisNeg,\n",
      "        ns1:LCMSAnalysisPos ;\n",
      "    ns1:has_lcms_feature_list ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisPos ;\n",
      "    ns1:has_sirius_annotation ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisPos ;\n",
      "    ns2:has_bioassay_results ns1:BioAssayResults,\n",
      "        ns2:L610ugml,\n",
      "        ns2:Ldono10ugml,\n",
      "        ns2:Ldono2ugml,\n",
      "        ns2:SwissTPHBioAssay,\n",
      "        ns2:Tbrucei10ugml,\n",
      "        ns2:Tbrucei2ugml,\n",
      "        ns2:Tcruzi10ugml .\n",
      "\n",
      "ns1:LabObject rdfs:label xsd:string ;\n",
      "    ns1:has_LCMS xsd:string,\n",
      "        ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisNeg,\n",
      "        ns1:LCMSAnalysisPos,\n",
      "        ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:has_lab_process ns1:LabExtract,\n",
      "        ns1:LabObject ;\n",
      "    ns1:has_lcms_feature_list xsd:string,\n",
      "        ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisPos,\n",
      "        ns1:LabExtract,\n",
      "        ns1:LabObject,\n",
      "        ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:has_sirius_annotation xsd:string,\n",
      "        ns1:LCMSAnalysis,\n",
      "        ns1:LCMSAnalysisPos,\n",
      "        ns1:LabExtract,\n",
      "        ns1:LabObject,\n",
      "        ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:has_unresolved_taxon [ ] ;\n",
      "    ns1:has_wd_id ns1:WDTaxon,\n",
      "        ns1:XRef ;\n",
      "    ns1:submitted_taxon xsd:string ;\n",
      "    ns2:has_bioassay_results ns1:BioAssayResults,\n",
      "        ns2:L610ugml,\n",
      "        ns2:Ldono10ugml,\n",
      "        ns2:Ldono2ugml,\n",
      "        ns2:SwissTPHBioAssay,\n",
      "        ns2:Tbrucei10ugml,\n",
      "        ns2:Tbrucei2ugml,\n",
      "        ns2:Tcruzi10ugml ;\n",
      "    ns2:has_broad_organe [ ] ;\n",
      "    ns2:has_organe [ ] ;\n",
      "    ns2:has_subsystem [ ] ;\n",
      "    ns2:has_tissue [ ] .\n",
      "\n",
      "ns1:spec2vec rdfs:label xsd:string ;\n",
      "    ns1:has_spec2vec_loss ns1:Spec2VecLoss,\n",
      "        ns1:spec2vec ;\n",
      "    ns1:has_spec2vec_peak ns1:Spec2VecPeak,\n",
      "        ns1:spec2vec ;\n",
      "    ns1:has_value xsd:float .\n",
      "\n",
      "ns1:LCMSAnalysis ns1:has_gnpslcms_link [ ] ;\n",
      "    ns1:has_lcms_feature_list ns1:LCMSFeatureList ;\n",
      "    ns1:has_massive_doi [ ] ;\n",
      "    ns1:has_massive_license [ ] ;\n",
      "    ns1:has_sirius_annotation ns1:LCMSFeatureList .\n",
      "\n",
      "ns2:ChEMBLTarget ns2:target_name xsd:string .\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################### Instantiate the graph #######################\n",
    "\n",
    "endpoint_url = 'https://enpkg.commons-lab.org/graphdb/repositories/ENPKG' #remove\n",
    "#create function put to main\n",
    "# init with the endpoint url\n",
    "graph = RdfGraph(\n",
    "    query_endpoint=endpoint_url,\n",
    "    standard=\"rdf\")\n",
    "\n",
    "# #OR\n",
    "\n",
    "# graph = RdfGraph(\n",
    "#     query_endpoint=endpoint_url,\n",
    "#     standard=\"rdf\",\n",
    "#     schema_file='../graphs/schema.ttl')\n",
    "\n",
    "\n",
    "# save the graph on disk\n",
    "with open('../graphs/graph.pkl', 'wb') as output_file:\n",
    "    pickle.dump(graph, output_file)\n",
    "    \n",
    "\n",
    "####################### Load the graph from disk #######################\n",
    "    \n",
    "# # load the graph from disk\n",
    "# with open('../graphs/graph.pkl', 'rb') as input_file:\n",
    "#     graph = pickle.load(input_file)\n",
    "    \n",
    "print(graph.get_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup: defining the temperature for the model and specifying model IDs for GPT-4 usage\n",
    "temperature = 0 # Temperature setting for the GPT models, influencing randomness in responses.\n",
    "model_id_gpt4 = \"gpt-4\" \n",
    "model_id = \"gpt-4-0125-preview\"\n",
    "\n",
    "# Create instances of the ChatOpenAI class for interacting with the GPT-4 models.\n",
    "# These instances are configured for specific model versions, with retries and verbose logging enabled.\n",
    "#defining gpt4 llm for supervisor\n",
    "llm_gpt4 = ChatOpenAI(temperature=temperature, \n",
    "                    model=model_id_gpt4, \n",
    "                    max_retries=3,\n",
    "                    verbose=True,\n",
    "                    ) # Instance for default GPT-4 model\n",
    "\n",
    "llm = ChatOpenAI(temperature=temperature, \n",
    "                    model=model_id, # This instance uses a specific GPT-4 turbo model.\n",
    "                    max_retries=3,\n",
    "                    verbose=True,\n",
    "                    ) # Instance for GPT-4 0125-preview model.\n",
    "\n",
    "\n",
    "# Setup a GraphSparqlQAChain instance for executing SPARQL queries against a knowledge graph.\n",
    "# This instance utilizes the llm model for processing and the graph object for data querying.\n",
    "sparql_chain = GraphSparqlQAChain.from_llm(\n",
    "    llm, graph=graph, verbose=True\n",
    ")\n",
    "\n",
    "# Initialize chemical and taxon resolver tools with the llm model for specialized query processing.\n",
    "chem_res = ChemicalResolver.from_llm(llm=llm, verbose=True)\n",
    "taxon_res = TaxonResolver()\n",
    "\n",
    "# Pydantic models for structured input to the resolver tools.\n",
    "class ChemicalInput(BaseModel):\n",
    "    query: str = Field(description=\"natural product compound string\")\n",
    "\n",
    "\n",
    "class SparqlInput(BaseModel):\n",
    "    question: str = Field(description=\"the original question from the user\")\n",
    "    entities: str = Field(description=\"strings containing for all entities, entity name and the corresponding entity identifier\")\n",
    "\n",
    "\n",
    "# Define a list of structured tools for chemical, taxon, target, and SMILES conversion resolution.\n",
    "#def tools_resolver_creator():\n",
    "tools_resolver = [\n",
    "        StructuredTool.from_function(\n",
    "            name = \"CHEMICAL_RESOLVER\",\n",
    "            func = chem_res.run,\n",
    "            description=\"The function takes a natural product compound string as input and returns a InChIKey, if InChIKey not found, it returns the NPCClass, NPCPathway or NPCSuperClass.\",\n",
    "            args_schema=ChemicalInput,\n",
    "        ),\n",
    "        StructuredTool.from_function(\n",
    "            name = \"TAXON_RESOLVER\",\n",
    "            func=taxon_res.query_wikidata,\n",
    "            description=\"The function takes a taxon string as input and returns the wikidata ID.\",\n",
    "        ),\n",
    "        StructuredTool.from_function(\n",
    "            name = \"TARGET_RESOLVER\",\n",
    "            func=target_name_to_target_id,\n",
    "            description=\"The function takes a target string as input and returns the ChEMBLTarget IRI.\",\n",
    "        ),\n",
    "        StructuredTool.from_function(\n",
    "            name = \"SMILE_CONVERTER\",\n",
    "            func=smiles_to_inchikey,\n",
    "            description=\"The function takes a SMILES string as input and returns the InChIKey notation of the molecule.\",\n",
    "        )\n",
    "        ]\n",
    "   # return tools_resolver\n",
    "\n",
    "\n",
    "# Define a list for the SPARQL query runner tool, used for executing knowledge graph queries.   \n",
    "tool_sparql = [  \n",
    "     StructuredTool.from_function(\n",
    "        name = \"SPARQL_QUERY_RUNNER\",\n",
    "        func=sparql_chain.run,\n",
    "        description=\"The agent resolve the user's question by querying the knowledge graph database. Input should be a question and the resolved entities in the question.\",\n",
    "        args_schema=SparqlInput,\n",
    "        # return_direct=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Generate a list of tool names for reference or display purposes.\n",
    "tool_names = [tool.name for tool in tools_resolver] # List of tool names from the resolver tools.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Helper Utilities to create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    \"\"\"\n",
    "    Creates an AgentExecutor with LLM, set of tools, and system prompt.\n",
    "\n",
    "    This function initializes a chat prompt template with a system message, placeholders for messages,\n",
    "    and an agent scratchpad. It then creates an agent using the specified LLM and tools, \n",
    "    and wraps this agent in an AgentExecutor for execution.\n",
    "\n",
    "    Parameters:\n",
    "    - llm (ChatOpenAI): The language model to be used by the agent for generating responses.\n",
    "    - tools (list): A list of tools (functions or utilities) that the agent can use to perform actions or generate responses.\n",
    "    - system_prompt (str): A string that provides initial instructions or information to the agent. This is used to set up the context for the agent's operations.\n",
    "\n",
    "    Returns:\n",
    "    - AgentExecutor: An executor object that manages the execution of the agent, allowing the agent to process input and use tools as defined.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Initialize a ChatPromptTemplate with a system message, placeholders for incoming messages, and an agent scratchpad.\n",
    "    # This template structures the input to the language model, integrating static and dynamic content.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create an agent using the provided language model, tools, and the structured prompt.\n",
    "    # This agent can interact with users, process input, and use tools based on the prompt template.\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "    # Initialize an AgentExecutor to manage and execute the agent's operations.\n",
    "    # The executor facilitates the interaction between the agent and the tools, handling execution logic.\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent Supervisor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of agent names that will be part of the supervisor system.\n",
    "members = [\"ENPKG_agent\", \"Sparql_query_runner\"]\n",
    "\n",
    "# Define the system prompt that outlines the role and responsibilities of the supervisor agent,\n",
    "# including instructions on how to delegate tasks to specialized agents based on the user's question.\n",
    "system_prompt = ( \"You are a supervisor. As the supervisor, your primary role is to coordinate the flow of information between agents and ensure the appropriate processing of the user question based on its content. You have access to a team of specialized agents: {members}\"\n",
    "\" Here is a list of steps to help you accomplish your role:\"\n",
    "\" - Analyse the user question and delegate functions to the specialized agents below if needed:\"\n",
    "\" If the question mentions any of the following entities: natural product compound, chemical name, taxon name, target, SMILES structure, or numerical value delegate the question to the  ENPKG_agent. ENPKG_agent would provide resolved entities needed to generate SPARQL query. For example if the question mentions either caffeine, or Desmodium heterophyllum call ENPKG_agent.\"\n",
    "\" If you have answers from the agent mentioned above, you provide those answers with the user question to the Sparql_query_runner.\"\n",
    "\" If the question does not mention chemical name, taxon name, target name, nor SMILES structure, delegate the question to the agent Sparql_query_runner. The Sparql_query_runner agent will perform further proccessing and answer the question.\"\n",
    "\" Once the Sparql_query_runner has completed its task and provided the answer, mark the process as FINISH. Do not call the Sparql_query_runner again.\"\n",
    "\" For example, the user provides the following question: For features from Melochia umbellata in PI mode with SIRIUS annotations, get the ones for which a feature in NI mode with the same retention time  has the same SIRIUS annotation. Since the question mentions Melochia umbellata you should firstly delegate it to the ENPKG_agent which would provide wikidata IRI with TAXON_RESOLVER tool, and then, you should delegate the question together with the output generated by ENPKG_agent to the Sparql_query_runner agent.\"\n",
    "\" Avoid calling the same agent if this agent has already been called previously and provided the answer. For example, if you have called ENPKG_agent and it provided InChIKey for chemical compound do not call this agent again. \"\n",
    "\" - Collect the answer from Sparql_query_runner and provide the final assembled response back to the user.\"\n",
    "\" Always tell the user the SPARQL query that have been returned by the Sparql_query_runner.\"\n",
    "\" If the agent does not provide the expected output mark the process as FINISH.\"\n",
    "\" Remember, your efficiency in routing the questions accurately and collecting responses is crucial for the seamless operation of our system. If you don't know the answer to any of the steps, please say explicitly and help the user by providing a query that you think will be better interpreted.\")\n",
    "\n",
    "\n",
    "# Function to create a team supervisor agent that routes tasks based on user questions.\n",
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
    "    \"\"\"\n",
    "    Configures and returns a supervisor agent setup with decision-making logic for task routing.\n",
    "    \n",
    "    The supervisor uses a provided language model (llm) to analyze user questions and decides whether to delegate\n",
    "    the question to specialized agents (members), or to mark the process as finished based on predefined criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    - llm (ChatOpenAI): The language model to be used for processing and routing decisions.\n",
    "    - system_prompt (str): A detailed prompt describing the supervisor's role and decision-making guidelines.\n",
    "    - members (list): A list of specialized agents available for task delegation.\n",
    "    \n",
    "    Returns:\n",
    "    - str: A configured prompt or agent setup that integrates routing logic for processing user questions.\n",
    "    \"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), members=\", \".join(members))\n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining separate agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message for the entity resolution agent (resolver) responsible for processing user questions.\n",
    "# This message includes instructions for the agent on how to handle different types of entities mentioned in questions.\n",
    "system_message_resolver = \"\"\"You are an entity resolution agent for the Sparql_query_runner.\n",
    "You have access to the following tools:\n",
    "{tool_names}\n",
    "You should analyze the question and provide resolved entities to the supervisor. Here is a list of steps to help you accomplish your role:\n",
    "If the question ask anything about any entities that could be natural product compound, find the relevant IRI to this chemical class using CHEMICAL_RESOLVER. Input is the chemical class name. For example, if salicin is mentioned in the question, provide its IRI using CHEMICAL_RESOLVER, input is salicin. \n",
    "\n",
    "If a taxon is mentionned, find what is its wikidata IRI with TAXON_RESOLVER. Input is the taxon name. For example, if the question mentions acer saccharum, you should provide it's wikidata IRI using TAXON_RESOLVER tool.\")\n",
    "\n",
    "If a target is mentionned, find the ChEMBLTarget IRI of the target with TARGET_RESOLVER. Input is the target name.\n",
    "\n",
    "If a SMILE structure is mentionned, find what is the InChIKey notation of the molecule with SMILE_CONVERTER. Input is the SMILE structure. For example, if there is a string with similar structure to CCC12CCCN3C1C4(CC3) in the question, provide it to SMILE_CONVERTER.\n",
    "        \n",
    "Give me units relevant to numerical values in this question. Return nothing if units for value is not provided.\n",
    "Be sure to say that these are the units of the quantities found in the knowledge graph.\n",
    "Here is the list of units to find:\n",
    "    \"retention time\": \"minutes\",\n",
    "    \"activity value\": null, \n",
    "    \"feature area\": \"absolute count or intensity\", \n",
    "    \"relative feature area\": \"normalized area in percentage\", \n",
    "    \"parent mass\": \"ppm (parts-per-million) for m/z\",\n",
    "    \"mass difference\": \"delta m/z\", \n",
    "    \"cosine\": \"score from 0 to 1. 1 = identical spectra. 0 = completely different spectra\"\n",
    "\n",
    "\n",
    " You are required to submit only the final answer to the supervisor.\n",
    "        \n",
    "\"\"\".format(tool_names=\"\\n\".join(tool_names))\n",
    "\n",
    "\n",
    "# Create an agent for entity resolution based on the instructions provided in `system_message_resolver`.\n",
    "enpkg_agent=create_agent(llm, tools_resolver, system_message_resolver)\n",
    "\n",
    "\n",
    "# Create an agent for running SPARQL queries based on user requests and resolved entities provided by other agents.\n",
    "sparql_query_agent = create_agent(llm, tool_sparql, \"You are sparql query runner, you take as input the user request and resolved entities provided by other agents, generate a SPARQL query, run it on the knowledge graph and answer the question using SPARQL_QUERY_RUNNER tool. Provide the answer to the question to the supervisor. You are required to submit only the final answer to the supervisor. If you could not get the answer, provide the SPARQL query generated.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "#function to define nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
    "\n",
    "#creating nodes for each agent\n",
    "enpkg_node= functools.partial(agent_node, agent=enpkg_agent, name=\"ENPKG_agent\")\n",
    "sparql_query_node= functools.partial(agent_node, agent=sparql_query_agent, name=\"Sparql_query_runner\")\n",
    "supervisor_agent=create_team_supervisor(llm_gpt4, \n",
    "                                        system_prompt, members)\n",
    "\n",
    "#creating the workflow and adding nodes to it\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"ENPKG_agent\", enpkg_node)\n",
    "workflow.add_node(\"Sparql_query_runner\",sparql_query_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "#connect all the edges in the graph\n",
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"ENPKG_agent\": \"ENPKG_agent\", \"Sparql_query_runner\": \"Sparql_query_runner\",  \"FINISH\": END}\n",
    ")\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "app= workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'How many features (pos ionization and neg ionization modes) have the same SIRIUS/CSI:FingerID and ISDB annotation by comparing the InCHIKey of the annotations?'\n",
    "q1_bis = 'How many features (pos ionization and neg ionization modes) have the same SIRIUS/CSI:FingerID and ISDB annotation by comparing the InCHIKey2D of the annotations?'\n",
    "q2 = 'Which extracts have features (pos ionization mode) annotated as the class, aspidosperma-type alkaloids, by CANOPUS with a probability score above 0.5, ordered by the decresing count of features as aspidosperma-type alkaloids? Group by extract.'\n",
    "q3 = 'Among the structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon , which ones contain an aspidospermidine substructure, CCC12CCCN3C1C4(CC3)C(CC2)NC5=CC=CC=C45?'\n",
    "q4 = 'Among the SIRIUS structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon, which ones are reported in the Tabernaemontana genus in Wikidata?'\n",
    "q5 = 'Which compounds have annotations with chembl assay results indicating reported activity against T. cruzi by looking at the cosmic, zodiac and taxo scores?'\n",
    "q5_bis = 'Which compounds have annotations with chembl assay results indicating reported activity against Trypanosoma cruzi by looking at the cosmic, zodiac and taxo scores?'\n",
    "q6 = 'Filter the pos ionization mode features of the Melochia umbellata taxon annotated as [M+H]+ by SIRIUS to keep the ones for which a feature in neg ionization mode is detected with the same retention time (+/- 3 seconds) and a mass corresponding to the [M-H]- adduct (+/- 5ppm).'\n",
    "q7 = 'For features from the Melochia umbellata taxon in pos ionization mode with SIRIUS annotations, get the ones for which a feature in neg ionization mode with the same retention time (+/- 3 seconds) has the same SIRIUS annotation by comparing the InCHIKey 2D. Return the features, retention times, and InChIKey2D'\n",
    "q8 = \"Which features were annotated as 'Tetraketide meroterpenoids' by SIRIUS, and how many such features were found for each species and plant part?\"\n",
    "q9 = \"What are all distinct submitted taxons for the extracts in the knowledge graph?\"\n",
    "q10 = \"What are the taxons, lab process and label (if one exists) for each sample? Sort by sample and then lab process\"\n",
    "q11 = \"Count all the species per family in the collection\"\n",
    "\n",
    "q12 = \"Taxons can be found in enpkg:LabExtract. Find the best URI of the Taxon in the context of this question : \\n Among the structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon , which ones contain an aspidospermidine substructure, CCC12CCCN3C1C4(CC3)C(CC2)NC5=CC=CC=C45?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def test_sparql_endpoint(endpoint):\n",
    "    \"\"\"\n",
    "    Tests the validity of a SPARQL endpoint by sending a simple ASK query.\n",
    "    Parameters:\n",
    "    - endpoint (str): The URL of the SPARQL endpoint to test.\n",
    "    Returns:\n",
    "    - bool: True if the endpoint is valid and responsive, False otherwise.\n",
    "    \"\"\"\n",
    "    test_query = {\"query\": \"ASK WHERE { ?s ?p ?o } LIMIT 1\"}\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    try:\n",
    "        response = requests.get(endpoint, params=test_query, headers=headers)\n",
    "        response.raise_for_status()  # Ensures HTTP request was successful\n",
    "        # Validate response format\n",
    "        if response.json() is not None:\n",
    "            return True\n",
    "        else:\n",
    "            print(\"The endpoint did not return a valid SPARQL result.\")\n",
    "            return False\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to connect to the endpoint: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=test_sparql_endpoint(endpoint_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:11,519 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'ENPKG_agent'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:13,786 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ChemicalResolver chain...\u001b[0m\n",
      "InChIKey not found, trying NPC Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:18,081 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 11:39:21,525 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 11:39:22,937 - INFO - Loading faiss.\n",
      "2024-02-26 11:39:22,945 - INFO - Successfully loaded faiss.\n",
      "2024-02-26 11:39:23,221 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 11:39:25,203 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:26,976 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENPKG_agent': {'messages': [HumanMessage(content='The class \"aspidosperma-type alkaloids\" is annotated with the IRI https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type, under the category NPCClass.', name='ENPKG_agent')]}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:33,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Sparql_query_runner'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:35,084 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphSparqlQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:51,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://enpkg.commons-lab.org/kg/>\n",
      "PREFIX ns2: <https://enpkg.commons-lab.org/module/>\n",
      "SELECT ?extract (COUNT(?feature) AS ?countFeatures)\n",
      "WHERE {\n",
      "    ?feature ns1:has_canopus_annotation ?annotation .\n",
      "    ?annotation ns1:has_canopus_npc_class <https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type> .\n",
      "    ?annotation ns1:has_canopus_npc_class_prob ?prob .\n",
      "    FILTER(?prob > 0.5) .\n",
      "    ?lcmsList ns1:has_lcms_feature ?feature .\n",
      "    ?lcmsList ns1:has_ionization \"pos\" .\n",
      "    ?extract ns1:has_lcms_feature_list ?lcmsList .\n",
      "}\n",
      "GROUP BY ?extract\n",
      "ORDER BY DESC(?countFeatures)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:39:52,773 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sparql_query_runner': {'messages': [HumanMessage(content='The extracts with features annotated as the class \"aspidosperma-type alkaloids\" by CANOPUS with a probability score above 0.5, ordered by the decreasing count of features as aspidosperma-type alkaloids, are as follows:\\n\\n1. Extract: [VGF152_B02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF152_B02_pos.mzXML) - Count of Features: 74\\n2. Extract: [VGF157_D02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF157_D02_pos.mzXML) - Count of Features: 11\\n3. Extract: [VGF147_B11_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF147_B11_pos.mzXML) - Count of Features: 10\\n4. Extract: [VGF153_C03_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF153_C03_pos.mzXML) - Count of Features: 7\\n5. Extract: [VGF157_E02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF157_E02_pos.mzXML) - Count of Features: 2\\n6. Extract: [VGF154_D02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF154_D02_pos.mzXML) - Count of Features: 2\\n7. Extract: [VGF147_A10_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF147_A10_pos.mzXML) - Count of Features: 2\\n8. Extract: [VGF140_F02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF140_F02_pos.mzXML) - Count of Features: 2\\n\\nAnd many others with a count of 1 feature each.', name='Sparql_query_runner')]}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:40:17,053 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in app.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=q2)\n",
    "        ]\n",
    "    },\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stream(app, q2):\n",
    "    try:\n",
    "        # Iterate over the stream from app.stream()\n",
    "        for s in app.stream(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=q2)  # Assuming q2 is the content of the message\n",
    "                ]\n",
    "            },\n",
    "            {\"recursion_limit\": 100},  # Additional options for the stream\n",
    "        ):\n",
    "            # Check if \"__end__\" is not in the stream output\n",
    "            if \"__end__\" not in s:\n",
    "                print(s)  # Print the stream output\n",
    "                print(\"----\")  # Print the delimiter\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:17:47,755 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'ENPKG_agent'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:17:49,669 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ChemicalResolver chain...\u001b[0m\n",
      "InChIKey not found, trying NPC Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:17:53,920 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:17:56,788 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:17:57,993 - INFO - Loading faiss.\n",
      "2024-02-26 15:17:58,001 - INFO - Successfully loaded faiss.\n",
      "2024-02-26 15:17:58,294 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "/Users/madinabekbergenova/anaconda3/envs/kgai/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "2024-02-26 15:18:00,676 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:18:01,803 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENPKG_agent': {'messages': [HumanMessage(content='The chemical class \"aspidosperma-type alkaloids\" is identified by the IRI https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type in the NPCClass.', name='ENPKG_agent')]}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:18:06,104 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Sparql_query_runner'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:18:08,280 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphSparqlQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:18:25,647 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://enpkg.commons-lab.org/kg/>\n",
      "PREFIX ns2: <https://enpkg.commons-lab.org/module/>\n",
      "SELECT ?extract (COUNT(?feature) AS ?countFeatures)\n",
      "WHERE {\n",
      "    ?feature ns1:has_canopus_annotation ?annotation .\n",
      "    ?annotation ns1:has_canopus_npc_class <https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type> .\n",
      "    ?annotation ns1:has_canopus_npc_class_prob ?prob .\n",
      "    FILTER(?prob > 0.5) .\n",
      "    ?feature ns1:has_ionization \"pos\" .\n",
      "    ?extract ns1:has_lcms_feature_list ?featureList .\n",
      "    ?featureList ns1:has_lcms_feature ?feature .\n",
      "}\n",
      "GROUP BY ?extract\n",
      "ORDER BY DESC(?countFeatures)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:18:26,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sparql_query_runner': {'messages': [HumanMessage(content='The extracts with features annotated as the class \"aspidosperma-type alkaloids\" by CANOPUS with a probability score above 0.5, ordered by the decreasing count of features as aspidosperma-type alkaloids, are as follows:\\n\\n1. Extract: [VGF152_B02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF152_B02_pos.mzXML) - Count of Features: 74\\n2. Extract: [VGF157_D02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF157_D02_pos.mzXML) - Count of Features: 11\\n3. Extract: [VGF147_B11_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF147_B11_pos.mzXML) - Count of Features: 10\\n4. Extract: [VGF153_C03_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF153_C03_pos.mzXML) - Count of Features: 7\\n5. Extract: [VGF157_E02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF157_E02_pos.mzXML) - Count of Features: 2\\n6. Extract: [VGF154_D02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF154_D02_pos.mzXML) - Count of Features: 2\\n7. Extract: [VGF147_A10_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF147_A10_pos.mzXML) - Count of Features: 2\\n8. Extract: [VGF140_F02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF140_F02_pos.mzXML) - Count of Features: 2\\n\\nAnd many others with a count of 1 feature each.', name='Sparql_query_runner')]}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:18:49,726 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "process_stream(app, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_langchain(app, question):\n",
    "    final_response = \"\"  # Initialize an empty string to hold the final response\n",
    "    for s in app.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=question)\n",
    "            ]\n",
    "        },\n",
    "        {\"recursion_limit\": 100},\n",
    "    ):\n",
    "        if \"__end__\" not in s:\n",
    "            # Assuming 's' contains a response you want to display,\n",
    "            # you can concatenate or process it as needed.\n",
    "            # This example assumes 's' is a dictionary with a 'text' key containing the response.\n",
    "            # Adjust the key according to your actual response structure.\n",
    "            final_response += s.get('text', '') + \"\\n\"\n",
    "        else:\n",
    "            break  # Exit the loop if an end condition is met\n",
    "    return final_response.strip()  # Return the final response, removing any trailing newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:41:18,222 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:41:20,095 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ChemicalResolver chain...\u001b[0m\n",
      "InChIKey not found, trying NPC Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:41:21,743 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:41:24,382 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:41:25,376 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:41:29,297 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:41:30,833 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphSparqlQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:41:52,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://enpkg.commons-lab.org/kg/>\n",
      "PREFIX ns2: <https://enpkg.commons-lab.org/module/>\n",
      "SELECT ?extract (COUNT(?feature) AS ?countFeatures)\n",
      "WHERE {\n",
      "  ?feature ns1:has_canopus_annotation ?annotation .\n",
      "  ?annotation ns1:has_canopus_npc_class <https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type> .\n",
      "  ?annotation ns1:has_canopus_npc_class_prob ?prob .\n",
      "  FILTER(?prob > 0.5) .\n",
      "  ?feature ns1:has_ionization \"pos\" .\n",
      "  ?extract ns1:has_lcms_feature_list ?featureList .\n",
      "  ?featureList ns1:has_lcms_feature ?feature .\n",
      "}\n",
      "GROUP BY ?extract\n",
      "ORDER BY DESC(?countFeatures)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:41:54,694 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:43:26,955 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result=get_response_from_langchain(app, q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stream(app, q2):\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    try:\n",
    "        # Iterate over the stream from app.stream()\n",
    "        for s in app.stream(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=q2)  # Assuming q2 is the content of the message\n",
    "                ]\n",
    "            },\n",
    "            {\"recursion_limit\": 100},  # Additional options for the stream\n",
    "        ):\n",
    "            # Check if \"__end__\" is not in the stream output\n",
    "            if \"__end__\" not in s:\n",
    "                results.append(s)  # Append the stream output to results list instead of printing\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return results  # Return the list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:51:12,673 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:51:16,260 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ChemicalResolver chain...\u001b[0m\n",
      "InChIKey not found, trying NPC Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:51:19,843 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:51:22,367 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:51:23,892 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:51:25,939 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:51:27,624 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:51:35,815 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:51:37,477 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphSparqlQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:51:57,831 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://enpkg.commons-lab.org/kg/>\n",
      "PREFIX ns2: <https://enpkg.commons-lab.org/module/>\n",
      "SELECT ?extract (COUNT(?feature) AS ?countFeatures)\n",
      "WHERE {\n",
      "  ?feature ns1:has_canopus_annotation ?annotation .\n",
      "  ?annotation ns1:has_canopus_npc_class <https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type> ;\n",
      "              ns1:has_canopus_npc_class_prob ?prob .\n",
      "  FILTER(?prob > 0.5)\n",
      "  ?extract ns1:has_lcms_feature_list ?featureList .\n",
      "  ?featureList ns1:has_lcms_feature ?feature .\n",
      "  ?feature ns1:has_ionization \"pos\" .\n",
      "}\n",
      "GROUP BY ?extract\n",
      "ORDER BY DESC(?countFeatures)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:51:59,156 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-02-26 15:52:40,259 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'supervisor': {'next': 'ENPKG_agent'}}, {'ENPKG_agent': {'messages': [HumanMessage(content='The chemical class \"aspidosperma-type alkaloids\" is identified by the IRI https://enpkg.commons-lab.org/kg/npc_Aspidosperma_type in the NPCClass.', name='ENPKG_agent')]}}, {'supervisor': {'next': 'Sparql_query_runner'}}, {'Sparql_query_runner': {'messages': [HumanMessage(content='The extracts with features annotated as the class \"aspidosperma-type alkaloids\" by CANOPUS with a probability score above 0.5, ordered by the decreasing count of features as aspidosperma-type alkaloids, are as follows:\\n\\n1. Extract: [VGF152_B02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF152_B02_pos.mzXML) - Count of Features: 74\\n2. Extract: [VGF157_D02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF157_D02_pos.mzXML) - Count of Features: 11\\n3. Extract: [VGF147_B11_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF147_B11_pos.mzXML) - Count of Features: 10\\n4. Extract: [VGF153_C03_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF153_C03_pos.mzXML) - Count of Features: 7\\n5. Extract: [VGF157_E02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF157_E02_pos.mzXML) - Count of Features: 2\\n6. Extract: [VGF154_D02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF154_D02_pos.mzXML) - Count of Features: 2\\n7. Extract: [VGF147_A10_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF147_A10_pos.mzXML) - Count of Features: 2\\n8. Extract: [VGF140_F02_pos.mzXML](https://enpkg.commons-lab.org/kg/VGF140_F02_pos.mzXML) - Count of Features: 2\\n9. Extract: [KP146_Positive.mzXML](https://enpkg.commons-lab.org/kg/KP146_Positive.mzXML) - Count of Features: 1\\n10. Extract: [KP317_Positive.mzXML](https://enpkg.commons-lab.org/kg/KP317_Positive.mzXML) - Count of Features: 1\\n\\n...and many more extracts each with a count of 1 feature as aspidosperma-type alkaloids.\\n\\nPlease note that the list above is truncated for brevity.', name='Sparql_query_runner')]}}, {'supervisor': {'next': 'FINISH'}}]\n"
     ]
    }
   ],
   "source": [
    "result=process_stream(app, q2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stream(question):\n",
    "    \"\"\"\n",
    "    Iterates over messages from app.stream(), printing each message until an \"__end__\" flag is encountered.\n",
    "\n",
    "    :param question: The question to be sent as part of the initial stream request.\n",
    "    \"\"\"\n",
    "    # Define the parameters for app.stream() as described\n",
    "    stream_params = {\n",
    "        \"messages\": [{\"content\": question}],\n",
    "    }\n",
    "    stream_options = {\"recursion_limit\": 100}\n",
    "\n",
    "    try:\n",
    "        for s in app.stream(stream_params, stream_options):\n",
    "            if \"__end__\" not in s:\n",
    "                print(s)\n",
    "                print(\"----\")\n",
    "            else:\n",
    "                break  # Exit the loop if \"__end__\" is encountered\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during streaming: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during streaming: Message dict must contain 'role' and 'content' keys, got {'content': 'Which extracts have features (pos ionization mode) annotated as the class, aspidosperma-type alkaloids, by CANOPUS with a probability score above 0.5, ordered by the decresing count of features as aspidosperma-type alkaloids? Group by extract.'}\n"
     ]
    }
   ],
   "source": [
    "result=process_stream(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 10:07:39,506 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Sparql_query_runner'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 10:07:41,303 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphSparqlQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madinabekbergenova/anaconda3/envs/kgai/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "2024-02-26 10:08:19,979 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://enpkg.commons-lab.org/kg/>\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "\n",
      "SELECT (COUNT(DISTINCT ?feature) AS ?numberOfFeatures)\n",
      "WHERE {\n",
      "  ?featurePos ns1:has_ionization \"pos\" ;\n",
      "              ns1:has_sirius_annotation ?siriusAnnotationPos ;\n",
      "              ns1:has_isdb_annotation ?isdbAnnotationPos .\n",
      "  ?siriusAnnotationPos ns1:has_InChIkey2D ?inChIkey2DPos .\n",
      "  ?isdbAnnotationPos ns1:has_InChIkey2D ?inChIkey2DPos .\n",
      "\n",
      "  ?featureNeg ns1:has_ionization \"neg\" ;\n",
      "              ns1:has_sirius_annotation ?siriusAnnotationNeg ;\n",
      "              ns1:has_isdb_annotation ?isdbAnnotationNeg .\n",
      "  ?siriusAnnotationNeg ns1:has_InChIkey2D ?inChIkey2DNeg .\n",
      "  ?isdbAnnotationNeg ns1:has_InChIkey2D ?inChIkey2DNeg .\n",
      "\n",
      "  FILTER(?inChIkey2DPos = ?inChIkey2DNeg)\n",
      "  BIND(CONCAT(STR(?featurePos), STR(?featureNeg)) AS ?feature)\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq1_bis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecursion_limit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__end__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kgai/lib/python3.11/site-packages/langgraph/pregel/__init__.py:617\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    616\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 617\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_stream_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kgai/lib/python3.11/site-packages/langchain_core/runnables/base.py:1494\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1494\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/kgai/lib/python3.11/site-packages/langgraph/pregel/__init__.py:348\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    341\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    342\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(proc\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m proc, \u001b[38;5;28minput\u001b[39m, config \u001b[38;5;129;01min\u001b[39;00m tasks_w_config\n\u001b[1;32m    344\u001b[0m ]\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_EXCEPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[1;32m    355\u001b[0m _interrupt_or_proceed(done, inflight, step)\n",
      "File \u001b[0;32m~/anaconda3/envs/kgai/lib/python3.11/concurrent/futures/_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m~/anaconda3/envs/kgai/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/kgai/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for s in app.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=q1_bis)\n",
    "        ]\n",
    "    },\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q12})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
