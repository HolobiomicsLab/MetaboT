{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: replace run() by invoke() in agent calling (run going to be deprecated in langchain 0.2.0)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Setting up the LangSmith\n",
    "# #For now, all runs will be stored in the \"KGBot Testing - GPT4\"\n",
    "# #If you want to separate the traces to have a better control of specific traces.\n",
    "# #Metadata as llm version and temperature can be obtaneid from traces. \n",
    "# import os\n",
    "# #from uuid import uuid4\n",
    "# #unique_id = uuid4().hex[0:8]\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"KGBot Testing - GPT4\" #Please update the name here if you want to create a new project for separating the traces. \n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "# #Check if the client was initialized\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains.llm import LLMChain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, List, Tuple, Union\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "import re\n",
    "\n",
    "from RdfGraphCustom import RdfGraph\n",
    "from smile_resolver import smiles_to_inchikey\n",
    "from chemical_resolver import ChemicalResolver\n",
    "from target_resolver import target_name_to_target_id\n",
    "from taxon_resolver import TaxonResolver\n",
    "\n",
    "from sparql import GraphSparqlQAChain\n",
    "\n",
    "from langchain.agents import  Tool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from typing import (\n",
    "    List,\n",
    "    Tuple\n",
    ")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Instantiate the graph #######################\n",
    "\n",
    "endpoint_url = 'https://enpkg.commons-lab.org/graphdb/repositories/ENPKG'\n",
    "\n",
    "# init with the endpoint url\n",
    "graph = RdfGraph(\n",
    "    query_endpoint=endpoint_url,\n",
    "    standard=\"rdf\")\n",
    "\n",
    "# #OR\n",
    "\n",
    "# graph = RdfGraph(\n",
    "#     query_endpoint=endpoint_url,\n",
    "#     standard=\"rdf\",\n",
    "#     schema_file='../graphs/schema.ttl')\n",
    "\n",
    "\n",
    "# save the graph on disk\n",
    "with open('../graphs/graph.pkl', 'wb') as output_file:\n",
    "    pickle.dump(graph, output_file)\n",
    "    \n",
    "\n",
    "####################### Load the graph from disk #######################\n",
    "    \n",
    "# # load the graph from disk\n",
    "# with open('../graphs/graph.pkl', 'rb') as input_file:\n",
    "#     graph = pickle.load(input_file)\n",
    "    \n",
    "print(graph.get_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0\n",
    "model_id_gpt4 = \"gpt-4\" \n",
    "model_id = \"gpt-4-0125-preview\"\n",
    "#defining gpt4 llm for supervisor\n",
    "llm_gpt4 = ChatOpenAI(temperature=temperature, \n",
    "                    model=model_id_gpt4, # default is 'gpt-3.5-turbo'\n",
    "                    max_retries=3,\n",
    "                    verbose=True,\n",
    "                    # model_kwargs={\n",
    "                    #     \"top_p\": 0.95,\n",
    "                    #     }\n",
    "                    )\n",
    "# https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html?highlight=chatopenai#\n",
    "llm = ChatOpenAI(temperature=temperature, \n",
    "                    model=model_id, # default is 'gpt-3.5-turbo'\n",
    "                    max_retries=3,\n",
    "                    verbose=True,\n",
    "                    # model_kwargs={\n",
    "                    #     \"top_p\": 0.95,\n",
    "                    #     }\n",
    "                    )\n",
    "\n",
    "#https://api.python.langchain.com/en/latest/_modules/langchain/chains/graph_qa/sparql.html#\n",
    "sparql_chain = GraphSparqlQAChain.from_llm(\n",
    "    llm, graph=graph, verbose=True\n",
    ")\n",
    "\n",
    "chem_res = ChemicalResolver.from_llm(llm=llm, verbose=True)\n",
    "taxon_res = TaxonResolver()\n",
    "\n",
    "class ChemicalInput(BaseModel):\n",
    "    query: str = Field(description=\"natural product compound string\")\n",
    "\n",
    "\n",
    "\n",
    "class SparqlInput(BaseModel):\n",
    "    question: str = Field(description=\"the original question from the user\")\n",
    "    entities: str = Field(description=\"strings containing for all entities, entity name and the corresponding entity identifier\")\n",
    "\n",
    "\n",
    "tools_resolver = [\n",
    "    StructuredTool.from_function(\n",
    "        name = \"CHEMICAL_RESOLVER\",\n",
    "        func = chem_res.run,\n",
    "        description=\"The function takes a natural product compound string as input and returns a InChIKey, if InChIKey not found, it returns the NPCClass, NPCPathway or NPCSuperClass.\",\n",
    "        args_schema=ChemicalInput,\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        name = \"TAXON_RESOLVER\",\n",
    "        func=taxon_res.query_wikidata,\n",
    "        description=\"The function takes a taxon string as input and returns the wikidata ID.\",\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        name = \"TARGET_RESOLVER\",\n",
    "        func=target_name_to_target_id,\n",
    "        description=\"The function takes a target string as input and returns the ChEMBLTarget IRI.\",\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        name = \"SMILE_CONVERTER\",\n",
    "        func=smiles_to_inchikey,\n",
    "        description=\"The function takes a SMILES string as input and returns the InChIKey notation of the molecule.\",\n",
    "    )\n",
    "    ]\n",
    "    \n",
    "tool_sparql = [  \n",
    "     StructuredTool.from_function(\n",
    "        name = \"SPARQL_QUERY_RUNNER\",\n",
    "        func=sparql_chain.run,\n",
    "        description=\"The agent resolve the user's question by querying the knowledge graph database. Input should be a question and the resolved entities in the question.\",\n",
    "        args_schema=SparqlInput,\n",
    "        # return_direct=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "tool_names = [tool.name for tool in tools_resolver]\n",
    "\n",
    "\n",
    "# chain.run(q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Helper Utilities to create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent Supervisor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"Resolver\", \"Sparql_query_runner\"]\n",
    "system_prompt = ( \"You are a supervisor. As the supervisor, your primary role is to coordinate the flow of information between agents and ensure the appropriate processing of the user question based on its content. You have access to a team of specialized agents: {members}\"\n",
    "\" Here is a list of steps to help you accomplish your role:\"\n",
    "\" - Analyse the user question and delegate functions to the specialized agents below if needed:\"\n",
    "\" If the question mentions any of the following entities: natural product compound, chemical name, taxon name, target, SMILES structure, or numerical value delegate the question to the Resolver agent. Resolver would provide resolved entities needed to generate SPARQL query. For example if the question mentions either caffeine, or Desmodium heterophyllum call Resolver agent.\"\n",
    "\" If you have answers from the agent mentioned above, you provide those answers with the user question to the Sparql_query_runner.\"\n",
    "\" If the question does not mention chemical name, taxon name, target name, nor SMILES structure, delegate the question to the agent Sparql_query_runner.\"\n",
    "\" Once the Sparql_query_runner has completed its task and provided the answer, mark the process as FINISH. Do not call the Sparql_query_runner again.\"\n",
    "\" For example, the user provides the following question: For features from Melochia umbellata in PI mode with SIRIUS annotations, get the ones for which a feature in NI mode with the same retention time  has the same SIRIUS annotation. Since the question mentions Melochia umbellata you should firstly delegate it to the Resolver which would provide wikidata IRI with TAXON_RESOLVER tool, and then, you should delegate the question together with the output generated by Resolver agent to the Sparql_query_runner agent.\"\n",
    "\" Avoid calling the same agent if this agent has already been called previously and provided the answer. For example, if you have called Resolver and it provided InChIKey for chemical compound do not call this agent again. \"\n",
    "\" - Collect the answer from Sparql_query_runner and provide the final assembled response back to the user.\"\n",
    "\" Always tell the user the SPARQL query that have been returned by the Sparql_query_runner.\"\n",
    "\" If the agent does not provide the expected output mark the process as FINISH.\"\n",
    "\" Remember, your efficiency in routing the questions accurately and collecting responses is crucial for the seamless operation of our system. If you don't know the answer to any of the steps, please say explicitly and help the user by providing a query that you think will be better interpreted.\")\n",
    "\n",
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
    "    \"\"\"An LLM-based router.\"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), members=\", \".join(members))\n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt for Resolver agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "system_message_resolver = \"\"\"You are an entity resolution agent for the Sparql_query_runner.\n",
    "You have access to the following tools:\n",
    "{tool_names}\n",
    "You should analyze the question and provide resolved entities to the supervisor. Here is a list of steps to help you accomplish your role:\n",
    "If the question ask anything about any entities that could be natural product compound, find the relevant IRI to this chemical class using CHEMICAL_RESOLVER. Input is the chemical class name. For example, if salicin is mentioned in the question, provide its IRI using CHEMICAL_RESOLVER, input is salicin. \n",
    "\n",
    "If a taxon is mentionned, find what is its wikidata IRI with TAXON_RESOLVER. Input is the taxon name. For example, if the question mentions acer saccharum, you should provide it's wikidata IRI using TAXON_RESOLVER tool.\")\n",
    "\n",
    "If a target is mentionned, find the ChEMBLTarget IRI of the target with TARGET_RESOLVER. Input is the target name.\n",
    "\n",
    "If a SMILE structure is mentionned, find what is the InChIKey notation of the molecule with SMILE_CONVERTER. Input is the SMILE structure. For example, if there is a string with similar structure to CCC12CCCN3C1C4(CC3) in the question, provide it to SMILE_CONVERTER.\n",
    "        \n",
    "Give me units relevant to numerical values in this question. Return nothing if units for value is not provided.\n",
    "Be sure to say that these are the units of the quantities found in the knowledge graph.\n",
    "Here is the list of units to find:\n",
    "    \"retention time\": \"minutes\",\n",
    "    \"activity value\": null, \n",
    "    \"feature area\": \"absolute count or intensity\", \n",
    "    \"relative feature area\": \"normalized area in percentage\", \n",
    "    \"parent mass\": \"ppm (parts-per-million) for m/z\",\n",
    "    \"mass difference\": \"delta m/z\", \n",
    "    \"cosine\": \"score from 0 to 1. 1 = identical spectra. 0 = completely different spectra\"\n",
    "\n",
    "\n",
    " You are required to submit only the final answer to the supervisor.\n",
    "        \n",
    "\"\"\".format(tool_names=\"\\n\".join(tool_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt_resolver = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message_resolver),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_resolver.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining separate agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolver_agent=create_agent(llm, tools_resolver, system_message_resolver)\n",
    "sparql_query_agent = create_agent(llm, tool_sparql, \"You are sparql query runner, you take as input the user request and resolved entities provided by other agents, generate a SPARQL query, run it on the knowledge graph and answer the question using SPARQL_QUERY_RUNNER tool. You are required to submit only the final answer to the supervisor. If you could not get the answer, provide the SPARQL query generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "#function to define nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
    "\n",
    "#creating nodes for each agent\n",
    "resolver_node= functools.partial(agent_node, agent=resolver_agent, name=\"Resolver\")\n",
    "sparql_query_node= functools.partial(agent_node, agent=sparql_query_agent, name=\"Sparql_query_runner\")\n",
    "supervisor_agent=create_team_supervisor(llm_gpt4, \n",
    "                                        system_prompt, members)\n",
    "\n",
    "#creating the workflow and adding nodes to it\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Resolver\", resolver_node)\n",
    "workflow.add_node(\"Sparql_query_runner\",sparql_query_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "#connect all the edges in the graph\n",
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"Resolver\": \"Resolver\", \"Sparql_query_runner\": \"Sparql_query_runner\",  \"FINISH\": END}\n",
    ")\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "app= workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'How many features (pos ionization and neg ionization modes) have the same SIRIUS/CSI:FingerID and ISDB annotation by comparing the InCHIKey of the annotations?'\n",
    "q1_bis = 'How many features (pos ionization and neg ionization modes) have the same SIRIUS/CSI:FingerID and ISDB annotation by comparing the InCHIKey2D of the annotations?'\n",
    "q2 = 'Which extracts have features (pos ionization mode) annotated as the class, aspidosperma-type alkaloids, by CANOPUS with a probability score above 0.5, ordered by the decresing count of features as aspidosperma-type alkaloids? Group by extract.'\n",
    "q3 = 'Among the structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon , which ones contain an aspidospermidine substructure, CCC12CCCN3C1C4(CC3)C(CC2)NC5=CC=CC=C45?'\n",
    "q4 = 'Among the SIRIUS structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon, which ones are reported in the Tabernaemontana genus in Wikidata?'\n",
    "q5 = 'Which compounds have annotations with chembl assay results indicating reported activity against T. cruzi by looking at the cosmic, zodiac and taxo scores?'\n",
    "q5_bis = 'Which compounds have annotations with chembl assay results indicating reported activity against Trypanosoma cruzi by looking at the cosmic, zodiac and taxo scores?'\n",
    "q6 = 'Filter the pos ionization mode features of the Melochia umbellata taxon annotated as [M+H]+ by SIRIUS to keep the ones for which a feature in neg ionization mode is detected with the same retention time (+/- 3 seconds) and a mass corresponding to the [M-H]- adduct (+/- 5ppm).'\n",
    "q7 = 'For features from the Melochia umbellata taxon in pos ionization mode with SIRIUS annotations, get the ones for which a feature in neg ionization mode with the same retention time (+/- 3 seconds) has the same SIRIUS annotation by comparing the InCHIKey 2D. Return the features, retention times, and InChIKey2D'\n",
    "q8 = \"Which features were annotated as 'Tetraketide meroterpenoids' by SIRIUS, and how many such features were found for each species and plant part?\"\n",
    "q9 = \"What are all distinct submitted taxons for the extracts in the knowledge graph?\"\n",
    "q10 = \"What are the taxons, lab process and label (if one exists) for each sample? Sort by sample and then lab process\"\n",
    "q11 = \"Count all the species per family in the collection\"\n",
    "\n",
    "q12 = \"Taxons can be found in enpkg:LabExtract. Find the best URI of the Taxon in the context of this question : \\n Among the structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon , which ones contain an aspidospermidine substructure, CCC12CCCN3C1C4(CC3)C(CC2)NC5=CC=CC=C45?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in app.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=q2)\n",
    "        ]\n",
    "    },\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = agent_executor.invoke({\"input\": q1_bis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q5_bis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": q12})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
