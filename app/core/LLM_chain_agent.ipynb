{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T14:25:49.535807Z",
     "start_time": "2024-03-26T14:25:49.522979Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "#load_dotenv()\n",
    "\n",
    "#Configuring the log system to work on Jupyter notebook\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Define the path to your log file\n",
    "log_file_path = 'kgbot.log'\n",
    "\n",
    "# Check if the log file already exists and delete it if it does\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "\n",
    "class StreamToLogger:\n",
    "    def __init__(self, logger, log_level=logging.INFO, echo=True):\n",
    "        self.logger = logger\n",
    "        self.log_level = log_level\n",
    "        self.echo = echo\n",
    "        self.original_stdout = sys.__stdout__  # Use sys.__stdout__ to ensure the original stdout\n",
    "        self._is_logging = False  # Guard to prevent recursion\n",
    "\n",
    "    def write(self, buf):\n",
    "        if self._is_logging:\n",
    "            return  # Avoid recursion\n",
    "\n",
    "        self._is_logging = True\n",
    "        try:\n",
    "            if buf.rstrip():\n",
    "                for line in buf.rstrip().splitlines():\n",
    "                    self.logger.log(self.log_level, line.rstrip())\n",
    "                    if self.echo:\n",
    "                        self.original_stdout.write(line + '\\n')\n",
    "        finally:\n",
    "            self._is_logging = False\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Set up the logging configuration\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "fh = logging.FileHandler(log_file_path, mode='a')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# Create a specific logger for stdout redirection\n",
    "stdout_logger = logging.getLogger('stdoutLogger')\n",
    "stdout_logger.setLevel(logging.INFO)\n",
    "stdout_logger.addHandler(fh)  # Add only the file handler to this logger\n",
    "\n",
    "# Redirect stdout to StreamToLogger\n",
    "stream_to_logger = StreamToLogger(stdout_logger, log_level=logging.INFO, echo=True)\n",
    "sys.stdout = stream_to_logger\n",
    "\n",
    "print(f\"Starting KGBot - {datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T14:25:50.185036Z",
     "start_time": "2024-03-26T14:25:50.177515Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Setting up the LangSmith\n",
    "# #For now, all runs will be stored in the \"KGBot Testing - GPT4\"\n",
    "# #If you want to separate the traces to have a better control of specific traces.\n",
    "# #Metadata as llm version and temperature can be obtained from traces.\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = (\n",
    "    f\"KGBot Testing - Interpreter_agent\"  # Please update the name here if you want to create a new project for separating the traces.\n",
    ")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# #Check if the client was initialized\n",
    "print(f\"Langchain client was initialized: {client}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:56:08.917047Z",
     "start_time": "2024-03-18T10:56:07.351667Z"
    }
   },
   "outputs": [],
   "source": [
    "# langchain imports for agent and prompt handling\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser \n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "# langgraph imports for prebuilt tool invocation\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# langchain_core imports for message handling and action schema\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, FunctionMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "\n",
    "# langchain output parser for OpenAI functions\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from codeinterpreterapi import CodeInterpreterSession, File\n",
    "# typing imports for type hinting\n",
    "from typing import Annotated, List, Tuple, Union,  Any, Dict, Optional, Sequence, TypedDict\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "# Standard library imports for JSON and regular expressions\n",
    "import json\n",
    "import re\n",
    "from importlib import reload\n",
    "import sparql  \n",
    "reload(sparql)\n",
    "# Custom imports for RDF graph manipulation and chemical, target, taxon, and SPARQL resolution\n",
    "from RdfGraphCustom import RdfGraph\n",
    "from smile_resolver import smiles_to_inchikey\n",
    "from chemical_resolver import ChemicalResolver\n",
    "from target_resolver import target_name_to_target_id\n",
    "from taxon_resolver import TaxonResolver\n",
    "from sparql import GraphSparqlQAChain\n",
    "from custom_sqlite_file import SqliteSaver\n",
    "from log_search import LogMemoryAccessTool\n",
    "\n",
    "# langchain pydantic for base model definitions\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# langchain tools for base, structured tool definitions, and tool decorators\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "# Standard library import for object serialization\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:56:26.594380Z",
     "start_time": "2024-03-18T10:56:09.856932Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################### Instantiate the graph #######################\n",
    "\n",
    "endpoint_url = 'https://enpkg.commons-lab.org/graphdb/repositories/ENPKG' #remove\n",
    "#create function put to main\n",
    "# init with the endpoint url\n",
    "graph = RdfGraph(\n",
    "    query_endpoint=endpoint_url,\n",
    "    standard=\"rdf\")\n",
    "\n",
    "# #OR\n",
    "\n",
    "# graph = RdfGraph(\n",
    "#     query_endpoint=endpoint_url,\n",
    "#     standard=\"rdf\",\n",
    "#     schema_file='../graphs/schema.ttl')\n",
    "\n",
    "\n",
    "# save the graph on disk\n",
    "with open('../graphs/graph.pkl', 'wb') as output_file:\n",
    "    pickle.dump(graph, output_file)\n",
    "    \n",
    "\n",
    "####################### Load the graph from disk #######################\n",
    "    \n",
    "# # load the graph from disk\n",
    "# with open('../graphs/graph.pkl', 'rb') as input_file:\n",
    "#     graph = pickle.load(input_file)\n",
    "    \n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:20:11.935844Z",
     "start_time": "2024-03-15T14:20:11.904246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initial setup: defining the temperature for the model and specifying model IDs for GPT-4 usage\n",
    "temperature = 0.3\n",
    "model_id_gpt4 = \"gpt-4\"\n",
    "model_id = \"gpt-4-0125-preview\"\n",
    "\n",
    "# Create instances of the ChatOpenAI class for interacting with the GPT-4 models.\n",
    "# These instances are configured for specific model versions, with retries and verbose logging enabled.\n",
    "# defining gpt4 llm for supervisor\n",
    "llm_gpt4 = ChatOpenAI(\n",
    "    temperature=temperature, model=model_id_gpt4, max_retries=3, verbose=True, model_kwargs={\n",
    "                        \"top_p\": 0.95,\n",
    "                        }\n",
    ")  # Instance for default GPT-4 model\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=temperature,\n",
    "    model=model_id,  # This instance uses a specific GPT-4 turbo model.\n",
    "    max_retries=3,\n",
    "    verbose=True,\n",
    ")  # Instance for GPT-4 0125-preview model.\n",
    "\n",
    "\n",
    "# Setup a GraphSparqlQAChain instance for executing SPARQL queries against a knowledge graph.\n",
    "# This instance utilizes the llm model for processing and the graph object for data querying.\n",
    "sparql_chain = GraphSparqlQAChain.from_llm(llm_gpt4, graph=graph, verbose=True)\n",
    "\n",
    "# Initialize chemical and taxon resolver tools with the llm model for specialized query processing.\n",
    "chem_res = ChemicalResolver.from_llm(llm=llm, verbose=True)\n",
    "taxon_res = TaxonResolver()\n",
    "\n",
    "\n",
    "# Pydantic models for structured input to the resolver tools.\n",
    "class ChemicalInput(BaseModel):\n",
    "    query: str = Field(description=\"natural product compound string\")\n",
    "\n",
    "\n",
    "class SparqlInput(BaseModel):\n",
    "    question: str = Field(description=\"the original question from the user\")\n",
    "    entities: str = Field(\n",
    "        description=\"strings containing for all entities, entity name and the corresponding entity identifier\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a list of structured tools for chemical, taxon, target, and SMILES conversion resolution.\n",
    "# def tools_resolver_creator():\n",
    "tools_resolver = [\n",
    "    StructuredTool.from_function(\n",
    "        name=\"CHEMICAL_RESOLVER\",\n",
    "        func=chem_res.run,\n",
    "        description=\"The function takes a natural product compound string as input and returns a InChIKey, if InChIKey not found, it returns the NPCClass, NPCPathway or NPCSuperClass.\",\n",
    "        args_schema=ChemicalInput,\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        name=\"TAXON_RESOLVER\",\n",
    "        func=taxon_res.query_wikidata,\n",
    "        description=\"The function takes a taxon string as input and returns the wikidata ID.\",\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        name=\"TARGET_RESOLVER\",\n",
    "        func=target_name_to_target_id,\n",
    "        description=\"The function takes a target string as input and returns the ChEMBLTarget IRI.\",\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        name=\"SMILE_CONVERTER\",\n",
    "        func=smiles_to_inchikey,\n",
    "        description=\"The function takes a SMILES string as input and returns the InChIKey notation of the molecule.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Define a list for the SPARQL query runner tool, used for executing knowledge graph queries.\n",
    "tool_sparql = [\n",
    "    StructuredTool.from_function(\n",
    "        name=\"SPARQL_QUERY_RUNNER\",\n",
    "        func=sparql_chain.run,\n",
    "        description=\"The agent resolve the user's question by querying the knowledge graph database. Input should be a question and the resolved entities in the question. The output is the answer to the question and path to the file containing the SPARQL output.\",\n",
    "        args_schema=SparqlInput,\n",
    "        # return_direct=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Generate a list of tool names for reference or display purposes.\n",
    "tool_names = [\n",
    "    tool.name for tool in tools_resolver\n",
    "]  # List of tool names from the resolver tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  Helper Utilities to create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:20:14.311971Z",
     "start_time": "2024-03-15T14:20:14.306761Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    \"\"\"\n",
    "    Creates an AgentExecutor with LLM, set of tools, and system prompt.\n",
    "\n",
    "    This function initializes a chat prompt template with a system message, placeholders for messages,\n",
    "    and an agent scratchpad. It then creates an agent using the specified LLM and tools,\n",
    "    and wraps this agent in an AgentExecutor for execution.\n",
    "\n",
    "    Parameters:\n",
    "    - llm (ChatOpenAI): The language model to be used by the agent for generating responses.\n",
    "    - tools (list): A list of tools (functions or utilities) that the agent can use to perform actions or generate responses.\n",
    "    - system_prompt (str): A string that provides initial instructions or information to the agent. This is used to set up the context for the agent's operations.\n",
    "\n",
    "    Returns:\n",
    "    - AgentExecutor: An executor object that manages the execution of the agent, allowing the agent to process input and use tools as defined.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a ChatPromptTemplate with a system message, placeholders for incoming messages, and an agent scratchpad.\n",
    "    # This template structures the input to the language model, integrating static and dynamic content.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create an agent using the provided language model, tools, and the structured prompt.\n",
    "    # This agent can interact with users, process input, and use tools based on the prompt template.\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "    # Initialize an AgentExecutor to manage and execute the agent's operations.\n",
    "    # The executor facilitates the interaction between the agent and the tools, handling execution logic.\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create Agent Supervisor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:20:15.387626Z",
     "start_time": "2024-03-15T14:20:15.383731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of agent names that will be part of the supervisor system.\n",
    "members = [\"ENPKG_agent\", \"Sparql_query_runner\", \"Interpreter_agent\"]\n",
    "\n",
    "# Define the system prompt that outlines the role and responsibilities of the supervisor agent,\n",
    "# including instructions on how to delegate tasks to specialized agents based on the user's question.\n",
    "system_prompt = \"\"\"You are a supervisor. As the supervisor, your primary role is to coordinate the flow of information between agents and ensure the appropriate processing of the user question based on its content. You have access to a team of specialized agents: {members}.\n",
    "\n",
    "Here is a list of steps to help you accomplish your role:\n",
    "\n",
    "Analyse the user question and delegate functions to the specialized agents below if needed:\n",
    "If the question mentions any of the following entities: natural product compound, chemical name, taxon name, target, SMILES structure, or numerical value delegate the question to the ENPKG_agent. ENPKG_agent would provide resolved entities needed to generate SPARQL query. For example if the question mentions either caffeine, or Desmodium heterophyllum call ENPKG_agent.\n",
    "\n",
    "If you have answers from the agent mentioned above, you provide those answers with the user question to the Sparql_query_runner.\n",
    "\n",
    "If the question does not mention chemical name, taxon name, target name, nor SMILES structure, delegate the question to the agent Sparql_query_runner. The Sparql_query_runner agent will perform further processing and provide the path containing the SPARQL output.\n",
    "\n",
    "If the Sparql_query_runner provides a SPARQL query and the path to the file containing the SPARQL output without directly providing the answer (implying that the answer is too long to be directly included), then delegate this information to the Interpreter_agent for further analysis and interpretation. Provide the Interpreter_agent with the question, SPARQL query, and the path to the file provided by the Sparql_query_runner. Await the Interpreter_agent's response for the final answer.\n",
    "\n",
    "Once the Interpreter_agent has completed its task mark the process as FINISH. Do not call the Interpreter_agent again.\n",
    "\n",
    "If the Sparql_query_runner agent provides a SPARQL query, the path to the file containing the SPARQL output and final answer to the question, and there is no immediate need for further interpretation, normally mark the process as FINISH. However, if there is a need to visualize the results (regardless of the length of the SPARQL output), also call the Interpreter_agent to generate the necessary plot, chart, or graph based on the SPARQL output. The need for visualization should be assessed based on the user's request or if the nature of the data implies that visualization would enhance understanding. Once the Interpreter_agent has completed its task mark the process as FINISH. Do not call the Interpreter_agent again.\n",
    "\n",
    "For example, the user provides the following question: For features from Melochia umbellata in PI mode with SIRIUS annotations, get the ones for which a feature in NI mode with the same retention time has the same SIRIUS annotation. Since the question mentions Melochia umbellata you should firstly delegate it to the ENPKG_agent which would provide wikidata IRI with TAXON_RESOLVER tool, then, you should delegate the question together with the output generated by ENPKG_agent to the Sparql_query_runner agent. Afterwards, if the Sparql_query_runner agent provided the answer to the question, SPARQL query and path to the file containing the SPARQL output and there is no need to visualize the output you should mark the process as FINISH. If the Sparql_query_runner agent  provided only SPARQL query and path to the file you should call Interpreter_agent which would interpret the results provided by Sparql_query_runner to generate the final response to the question.\n",
    "\n",
    "Avoid calling the same agent if this agent has already been called previously and provided the answer. For example, if you have called ENPKG_agent and it provided InChIKey for chemical compound do not call this agent again.\n",
    "\n",
    "Always tell the user the SPARQL query that has been returned by the Sparql_query_runner.\n",
    "\n",
    "If the agent does not provide the expected output mark the process as FINISH.\n",
    "\n",
    "Remember, your efficiency in routing the questions accurately and collecting responses is crucial for the seamless operation of our system. If you don't know the answer to any of the steps, please say explicitly and help the user by providing a query that you think will be better interpreted.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Function to create a team supervisor agent that routes tasks based on user questions.\n",
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
    "    \"\"\"\n",
    "    Configures and returns a supervisor agent setup with decision-making logic for task routing.\n",
    "\n",
    "    The supervisor uses a provided language model (llm) to analyze user questions and decides whether to delegate\n",
    "    the question to specialized agents (members), or to mark the process as finished based on predefined criteria.\n",
    "\n",
    "    Parameters:\n",
    "    - llm (ChatOpenAI): The language model to be used for processing and routing decisions.\n",
    "    - system_prompt (str): A detailed prompt describing the supervisor's role and decision-making guidelines.\n",
    "    - members (list): A list of specialized agents available for task delegation.\n",
    "\n",
    "    Returns:\n",
    "    - str: A configured prompt or agent setup that integrates routing logic for processing user questions.\n",
    "    \"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), members=\", \".join(members))\n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining separate agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:20:16.625162Z",
     "start_time": "2024-03-15T14:20:16.620628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the system message for the entity resolution agent (resolver) responsible for processing user questions.\n",
    "# This message includes instructions for the agent on how to handle different types of entities mentioned in questions.\n",
    "system_message_resolver = \"\"\"You are an entity resolution agent for the Sparql_query_runner. If this prompt has reached you, it is for a specific and significant reason. It is imperative that you read the instructions and details provided carefully and thoroughly.\n",
    "You have access to the following tools:\n",
    "{tool_names}\n",
    "You should analyze the question and provide resolved entities for the entities mentioned in the question to the supervisor using the tools available to you. Here is a list of steps to help you accomplish your role:\n",
    "\n",
    "If the question ask anything about any entities that could be natural product compound, find the relevant IRI to this chemical class using CHEMICAL_RESOLVER tool. Input is the chemical class name. For example, if salicin is mentioned in the question, provide its IRI using CHEMICAL_RESOLVER, input is salicin. \n",
    "\n",
    "If a taxon is mentioned, find what is its wikidata IRI with TAXON_RESOLVER. Input is the taxon name. For example, if the question mentions acer saccharum, you should provide its wikidata IRI using TAXON_RESOLVER tool. \n",
    "\n",
    "If a target is mentioned, find the ChEMBLTarget IRI of the target using TARGET_RESOLVER tool. Input is the target name.\n",
    "\n",
    "If a SMILE structure is mentioned, find what is the InChIKey notation of the molecule using SMILE_CONVERTER tool. Input is the SMILE structure. For example, if there is a string with similar structure to CCC12CCCN3C1C4(CC3) in the question, provide it to SMILE_CONVERTER.\n",
    "        \n",
    "Give me units relevant to numerical values in this question. Return nothing if units for value is not provided.\n",
    "Be sure to say that these are the units of the quantities found in the knowledge graph.\n",
    "Here is the list of units to find:\n",
    "    \"retention time\": \"minutes\",\n",
    "    \"activity value\": null, \n",
    "    \"feature area\": \"absolute count or intensity\", \n",
    "    \"relative feature area\": \"normalized area in percentage\", \n",
    "    \"parent mass\": \"ppm (parts-per-million) for m/z\",\n",
    "    \"mass difference\": \"delta m/z\", \n",
    "    \"cosine\": \"score from 0 to 1. 1 = identical spectra. 0 = completely different spectra\"\n",
    "\n",
    " You do not have to answer the question, your task is to provide resolved entities mentioned in the question using the tools available to you. For example, if the question mentions Hibiscus rosa-sinensis provide its wikidata IRI using TAXON_RESOLVER tool.\n",
    " \n",
    " You are required to submit only the final answer to the supervisor.\n",
    "        \n",
    "\"\"\".format(\n",
    "    tool_names=\"\\n\".join(tool_names)\n",
    ")\n",
    "\n",
    "\n",
    "# Create an agent for entity resolution based on the instructions provided in `system_message_resolver`.\n",
    "enpkg_agent = create_agent(llm_gpt4, tools_resolver, system_message_resolver)\n",
    "\n",
    "\n",
    "# Create an agent for running SPARQL queries based on user requests and resolved entities provided by other agents.\n",
    "system_message_sparql = \"\"\"You are SPARQL query runner, you take as input the user request and resolved entities provided by other agents, generate a SPARQL query, run it on the knowledge graph and answer to the question using SPARQL_QUERY_RUNNER tool. Specifically, when providing user request and resolved entities to the SPARQL_QUERY_RUNNER tool, format them as 'entity from the question has entity type entity resolution'. For example, you should provide the following input: catharanthus roseus has the Wikidata IRI https://www.wikidata.org/wiki/Q161093. Ensure this format is strictly adhered to for effective querying.  \n",
    "\n",
    "If the output of the SPARQL_QUERY_RUNNER tool consists of only generated SPARQL query and path to the file containing the SPARQL output, you will need to generate a dictionary as output from your process. This dictionary should contain exactly three key-value pairs:\n",
    "question: The key should be a string named 'question' and the value should be the natural language question you were asked to translate into a SPARQL query.\n",
    "generated_sparql_query: The key should be a string named 'generated_sparql_query' and the value should be the SPARQL query you generated based on the natural language question.\n",
    "file_path: The key should be a string named 'file_path' and the value should be the absolute path to the file where the generated SPARQL query is saved. In this case provide the generated dictionary to the supervisor which would call the Interpreter agent to further interpret the results.\n",
    "\n",
    "If the output of the SPARQL_QUERY_RUNNER tool consists of generated SPARQL query, path to the file containing the SPARQL output and the SPARQL output then you need to generate the final answer to the question based on the SPARQL output. Provide the final answer to the question together with the dictionary containing the question, generated_sparql_query and file_path. The dictionary should contain exactly three key-value pairs:\n",
    "question: The key should be a string named 'question' and the value should be the natural language question you were asked to translate into a SPARQL query.\n",
    "generated_sparql_query: The key should be a string named 'generated_sparql_query' and the value should be the SPARQL query you generated based on the natural language question.\n",
    "file_path: The key should be a string named 'file_path' and the value should be the absolute path to the file where the generated SPARQL query is saved.\n",
    " Provide the final answer to the supervisor.\n",
    "\"\"\"\n",
    "\n",
    "sparql_query_agent = create_agent(llm, tool_sparql, system_message_sparql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining interpreter agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T14:26:12.179256Z",
     "start_time": "2024-03-26T14:26:12.155240Z"
    }
   },
   "outputs": [],
   "source": [
    "interpreter_session = CodeInterpreterSession()\n",
    "\n",
    "\n",
    "class InterpreterInput(BaseModel):\n",
    "    question: str = Field(description=\"the original question from the user\")\n",
    "    generated_sparql_query: str = Field(description=\"the generated SPARQL query\")\n",
    "    file_path: str = Field(\n",
    "        description=\"file path where result of generated SPARQL query is stored\"\n",
    "    )\n",
    "\n",
    "\n",
    "def interpreter_logic(question, generated_sparql_query, file_path) -> None:\n",
    "    \"\"\"Interprets the results of a SPARQL query based on user's question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The original question from the user.\n",
    "        generated_sparql_query (str): The generated SPARQL query.\n",
    "        file_path (str): The file path where the result of the generated SPARQL query is stored.\n",
    "\n",
    "    Returns:\n",
    "        None: Outputs the response after interpreting the SPARQL results.\n",
    "    \"\"\"\n",
    "    # context manager for start/stop of the session\n",
    "    # define the user request\n",
    "    print(f\"Interpreting {question}\")\n",
    "    print(f\"SPARQL query: {generated_sparql_query}\")\n",
    "    print(f\"File path: {file_path}\")\n",
    "    with CodeInterpreterSession() as session:\n",
    "        user_request = f\"\"\"You are an interpreter agent. Your task is to analyze the output related to a SPARQL query, which could be in two forms:\n",
    "         If the output of the Sparql_query_runner agent is only the dictionary containing question: \"{question}\", generated SPARQL query: \"{generated_sparql_query}\" which was used to query the knowledge graph to answer to the question and path: \"{file_path}\" containing the SPARQL output then you should review the provided dataset from the file and SPARQL query to provide a clear, concise answer to the question. Additionally, if visualization of the results is necessary (e.g., when the SPARQL output is large or complex), you should provide an appropriate visualization, such as a bar chart, diagram, or plot, to effectively communicate the answer.\n",
    "         If the output of the Sparql_query_runner agent contains the answer to the question together with the dictionary containing the question: \"{question}\", generated SPARQL query: \"{generated_sparql_query}\" and path: \"{file_path}\", then you should analyze this output and provide visualization of the answer to the question. \n",
    "         The type of visualization – bar chart, diagram, or plot – will depend on the nature of the SPARQL output and the best way to represent the answer to the question clearly.\n",
    "         Submit only the final answer to the supervisor. Indicate the format of the dataset for appropriate handling. \"\"\"\n",
    "        files = [\n",
    "            File.from_path(file_path),\n",
    "        ]\n",
    "\n",
    "        # generate the response\n",
    "        response = session.generate_response(user_request, files=files)\n",
    "        # output the response (text + image)\n",
    "        response.show()\n",
    "        return response.content\n",
    "\n",
    "\n",
    "interpreter_tool = StructuredTool.from_function(\n",
    "    name=\"INTERPRETER_TOOL\",\n",
    "    func=interpreter_logic,\n",
    "    description=\"The function takes an original user question, generated sparql query, and generated sparql query result stored in file_path and returns interpreted answer content\",\n",
    "    args_schema=InterpreterInput,\n",
    ")\n",
    "system_message_interpreter = \"\"\"You are an interpreter agent. Your main role is to analyze outputs from the Sparql_query_runner agent using the INTERPRETER_TOOL. The outputs from the Sparql_query_runner agent can be of two types:\n",
    "\n",
    "The output is a dictionary containing 'question', 'generated_sparql_query', and 'file_path'. This typically happens when the Sparql_query_runner agent has executed a query to fetch results for a complex question. Your task is to provide this dictionary directly to the INTERPRETER_TOOL to get a concise answer. Ensure you format the dictionary correctly and include all necessary information so the INTERPRETER_TOOL can process it efficiently.\n",
    "\n",
    "The output directly contains the answer to the question but still comes within a dictionary that includes the 'question', 'generated_sparql_query', and 'file_path'. Even if the answer is directly provided, your role remains to pass this entire dictionary to the INTERPRETER_TOOL. The tool requires this structured input to validate and format the final answer properly.\n",
    "\n",
    "In both scenarios, your primary function is to ensure that the INTERPRETER_TOOL receives the necessary information in a structured dictionary format. This allows the tool to analyze the SPARQL query's output thoroughly and provide a clear, concise answer to the initial question.\"\"\"\n",
    "\n",
    "interpreter_agent = create_agent(llm, [interpreter_tool], system_message_interpreter)\n",
    "\n",
    "print(\"Interpreter agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Entry Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryInput(BaseModel):\n",
    "    query: str = Field(description=\"Query string to search in memory logs.\")\n",
    "\n",
    "def memory_access_tool_creator():\n",
    "    # Function adjusted to accept keyword arguments\n",
    "    def memory_tool(**kwargs) -> Dict[str, Any]:\n",
    "        # Create a QueryInput instance from kwargs\n",
    "        query_input = QueryInput(**kwargs)\n",
    "        \n",
    "        # Instantiate LogMemoryAccessTool with its default parameters\n",
    "        new_memory_tool_instance = LogMemoryAccessTool()\n",
    "\n",
    "        # Directly use the generated answer method since we're focusing on generating responses\n",
    "        return new_memory_tool_instance.generate_answer(query_input=query_input)\n",
    "\n",
    "    # Assuming StructuredTool and its usage is similar to how you'd implement it based on your framework or requirements\n",
    "    new_memory_access_tool = StructuredTool.from_function(\n",
    "        name=\"NEW_MEMORY_ACCESS_QUERY_RUNNER\",\n",
    "        func=memory_tool,\n",
    "        description=\"Generates an answer based on the logs and the provided query without explicitly calling the input.\",\n",
    "        args_schema=QueryInput,  # Ensure this matches your expected input schema\n",
    "    )\n",
    "    \n",
    "    return new_memory_access_tool\n",
    "\n",
    "tool_memory = memory_access_tool_creator()\n",
    "\n",
    "# Creating the Entry Agent prompt\n",
    "# 1. Determine if the question is within the knowledge domain of our system, which includes chemistry, natural products chemistry, mass spectrometry, biology, metabolomics, knowledge graphs, and related areas.\n",
    "entry_agent_promtp = \"\"\"\n",
    "You are the first point of contact for user questions in a team of LLMs designed to answer technical questions involving the retrieval and interpretation of information from a Knowledge Graph Database of LC-MS Metabolomics of Natural Products. As the entry agent, you need to be very succint in your communications and answer only what you are instructed to. You should not answer questions out of your role. Your replies will be used by other LLMs as imputs, so it should strictly contain only what you are instructed to do.  \n",
    "\n",
    "Your role is to interpret the question sent by the user to you and to identify if the question is a \"New Knowledge Question\", a clarification you asked for a New Knowledge Question or a \"Help me understand Question\" and take actions based on this.\n",
    "\n",
    "A New Knowledge Question would be a question that requires information that you don't have available information at the moment and are not asking to explain results from previous questions. Those questions should be contained in the domains of Metabolomics, Chemistry, Mass Spectometry, Biology and Natural Products chemistry, and can include, for example, asking about compounds in a certain organism, to select and count the number of features containing a chemical entity, etc. If you identify that the question sent is a New Knowledge Question, you have to do the following:\n",
    "\n",
    "1. Check if the question requires clarification, focusing on these considerations:\n",
    "    - ONLY IF common usual names are mentioned, there is need for clarification on the specific species or taxa, as common names could refer to multiple entities. Some examples are provided:\n",
    "    -> The question \"How many compounds annotated in positive mode in the extracts of mint contain a benzene substructure?\" needs clarification since mint could refer to several species of the Mentha genus.\n",
    "    -> The question \"Select all compounds annotated in positive mode containing a benzene substructure\" don't need specification, since it implies that it whishes to select all compounds containing the benzene substructure from all organisms.\n",
    "    - ONLY IF the question includes unfinished scientific taxa specification, there is need for clarification only if the question implies specificity is needed. Some examples are provided:\n",
    "    -> The question \"Select all compounds from the genus Cedrus\" don't need clarification since it is already specifying that wants all species in the Cedrus genus.\n",
    "    -> The question \"Which species of Arabidopsis contains more compounds annotated in negative mode in the database?\" don't need clarification since it wants to compare all species from the genus Arabidopsis.\n",
    "    -> The question \"What compounds contain a spermidine substructure from Arabdopsis?\" needs clarification since it don't implies that wants the genus and also don't specify the species. \n",
    "    - For questions involving ionization mode without specification, ask whether positive or negative mode information is sought, as the database separates these details. If no ionization mode is specified, this implies that the question is asking for both positive and negative ionization mode. \n",
    "    - Remember: If the question does not mention a specific taxa and the context does not imply a need for such specificity, assume the question is asking for all taxa available in the database. There is no need for clarification in such cases.\n",
    "    - Similarly, if a chemical entity isn't specified, assume the query encompasses all chemical entities within the scope of the question. \n",
    "\n",
    "2. If you detected that there's need for clarification, you have to reply what information do you want to be more precise. If there's no need for clarification, reply \"Starting the processing of the question\"\n",
    "3. When the user clarified your previous doubt, you have to now reply the original question and the clarification, as your answer will be used by the next LLM.\n",
    "\n",
    "\n",
    "A \"Help me understand Question\" would be a follow up question, asking for explaining or providing more information about any previous answer. In this case, you have to:  \n",
    "\n",
    "1. Utilize previous conversations stored in the your memory for context when replyng to it, enabling more informed explanation about previous answers. If there's no information about it in your previous interactions, you should invoke your tool {tool} to search for information on the log. The input for the tool is what you want to search in the log. Use the answer given by the tool to help you reply back to the user. If there's also no information in the log, just reply that you don't have the information the user is looking for.\n",
    "\n",
    "You can also identify the need for transforming a \"Help me understand question\" in to a \"New Knowledge Question\". This would be a specific case when the user wants a explanation for a previous answer, but this explanation needs new information, that has to be searched on the database. In this case, you can formulate a question to be searched in the database based on previous conversation and the new information needed. \n",
    "\n",
    "If the question is outside of your knowledge or scope, don't reply anything. Other members of your team will tackle the issue.\n",
    "\n",
    "\"\"\".format(tool=tool_memory.name)\n",
    "\n",
    "entry_agent = create_agent(llm_gpt4, [tool_memory], entry_agent_promtp)\n",
    "\n",
    "print(\"Entry agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:20:21.071452Z",
     "start_time": "2024-03-15T14:20:21.065227Z"
    }
   },
   "outputs": [],
   "source": [
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "# function to define nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
    "\n",
    "\n",
    "# creating nodes for each agent\n",
    "entry_node= functools.partial(agent_node, agent=entry_agent, name=\"Entry_Agent\")\n",
    "enpkg_node = functools.partial(agent_node, agent=enpkg_agent, name=\"ENPKG_agent\")\n",
    "sparql_query_node = functools.partial(\n",
    "    agent_node, agent=sparql_query_agent, name=\"Sparql_query_runner\"\n",
    ")\n",
    "interpreter_agent_node = functools.partial(\n",
    "    agent_node, agent=interpreter_agent, name=\"Interpreter_agent\"\n",
    ")\n",
    "supervisor_agent = create_team_supervisor(llm_gpt4, system_prompt, members)\n",
    "\n",
    "# creating the workflow and adding nodes to it\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Entry_Agent\", entry_node)\n",
    "workflow.add_node(\"ENPKG_agent\", enpkg_node)\n",
    "workflow.add_node(\"Sparql_query_runner\", sparql_query_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"Interpreter_agent\", interpreter_agent_node)\n",
    "\n",
    "#Adding edges\n",
    "\n",
    "# Connecting Entry agent and supervisor\n",
    "workflow.add_edge(\"Entry_Agent\", \"supervisor\")\n",
    "\n",
    "# connect all the edges in the graph\n",
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"ENPKG_agent\": \"ENPKG_agent\",\n",
    "        \"Sparql_query_runner\": \"Sparql_query_runner\",\n",
    "        \"Interpreter_agent\": \"Interpreter_agent\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "memory = SqliteSaver()\n",
    "\n",
    "workflow.set_entry_point(\"Entry_Agent\")\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"KGBot was compiled using LangGraph and it's ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:20:25.975134Z",
     "start_time": "2024-03-15T14:20:25.959673Z"
    }
   },
   "outputs": [],
   "source": [
    "q1 = \"How many features (pos ionization and neg ionization modes) have the same SIRIUS/CSI:FingerID and ISDB annotation by comparing the InCHIKey of the annotations?\"\n",
    "q1_bis = \"How many features (pos ionization and neg ionization modes) have the same SIRIUS/CSI:FingerID and ISDB annotation by comparing the InCHIKey2D of the annotations?\"\n",
    "q2 = \"Which extracts have features (pos ionization mode) annotated as the class, aspidosperma-type alkaloids, by CANOPUS with a probability score above 0.5, ordered by the decreasing count of features as aspidosperma-type alkaloids? Group by extract.\"\n",
    "q3 = \"Among the structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon , which ones contain an aspidospermidine substructure?\"\n",
    "q4 = \"Among the SIRIUS structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract, which ones are reported in the Tabernaemontana genus in Wikidata?\"\n",
    "q5 = \"Which compounds have annotations with chembl assay results indicating reported activity against T. cruzi by looking at the cosmic, zodiac and taxo scores?\"\n",
    "q5_bis = \"Which compounds have annotations with chembl assay results indicating reported activity against Trypanosoma cruzi by looking at the cosmic, zodiac and taxo scores?\"\n",
    "q6 = \"Filter the pos ionization mode features of the Melochia umbellata taxon annotated as [M+H]+ by SIRIUS to keep the ones for which a feature in neg ionization mode is detected with the same retention time (+/- 3 seconds) and a mass corresponding to the [M-H]- adduct (+/- 5ppm).\"\n",
    "q7 = \"For features from the Melochia umbellata taxon in pos ionization mode with SIRIUS annotations, get the ones for which a feature in neg ionization mode with the same retention time (+/- 3 seconds) has the same SIRIUS annotation by comparing the InCHIKey 2D. Return the features, retention times, and InChIKey2D\"\n",
    "q8 = \"Which features were annotated as 'Tetraketide meroterpenoids' by SIRIUS, and how many such features were found for each species and plant part?\"\n",
    "q9 = \"What are all distinct submitted taxons for the extracts in the knowledge graph?\"\n",
    "q10 = \"What are the taxons, lab process and label (if one exists) for each sample? Sort by sample and then lab process\"\n",
    "q11 = \"Count all the species per family in the collection\"\n",
    "\n",
    "q12 = \"Taxons can be found in enpkg:LabExtract. Find the best URI of the Taxon in the context of this question : \\n Among the structural annotations from the Tabernaemontana coffeoides (Apocynaceae) seeds extract taxon , which ones contain an aspidospermidine substructure, CCC12CCCN3C1C4(CC3)C(CC2)NC5=CC=CC=C45?\"\n",
    "q13 = \"Which compounds annotated in the active extract of Melochia umbellata have activity against Trypanosoma cruzi reported (in ChEMBL)?\"\n",
    "q14 = \"What are the variations in the concentration of key active compounds found in Tabernaemontana coffeoides seed extracts across different sample collections?\"\n",
    "q15 = \"Which compounds are detected most in Tabernaemontana genus?\"\n",
    "q16 = (\n",
    "    \"What are the most frequently detected compounds in the leaves of the Tabernaemontana genus? \"\n",
    "    \" over of features annotated as certain chemical classes vary across different Tabernaemontana genus extracts in the ENPKG, with a focus on features identified in positive ionization mode and annotated by CANOPUS with a probability score above 0.5?\"\n",
    ")\n",
    "q17 = \" For all the plant extracts plot the distribution of number of features per sample retention time vs mass to charge ratio\"\n",
    "q18 = \"What are the most frequently observed chemical compounds in Tabernaemontana genus? Provide a bar chart.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T09:57:35.184809Z",
     "start_time": "2024-03-12T09:57:35.182143Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def test_sparql_endpoint(endpoint):\n",
    "    \"\"\"\n",
    "    Tests the validity of a SPARQL endpoint by sending a simple ASK query.\n",
    "    Parameters:\n",
    "    - endpoint (str): The URL of the SPARQL endpoint to test.\n",
    "    Returns:\n",
    "    - bool: True if the endpoint is valid and responsive, False otherwise.\n",
    "    \"\"\"\n",
    "    test_query = {\"query\": \"ASK WHERE { ?s ?p ?o } LIMIT 1\"}\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    try:\n",
    "        response = requests.get(endpoint, params=test_query, headers=headers)\n",
    "        response.raise_for_status()  # Ensures HTTP request was successful\n",
    "        # Validate response format\n",
    "        if response.json() is not None:\n",
    "            print(\"Succesfully connected to the endpoint\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"The endpoint did not return a valid SPARQL result.\")\n",
    "            return False\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to connect to the endpoint: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:57:35.997845Z",
     "start_time": "2024-03-12T09:57:35.921363Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = test_sparql_endpoint(endpoint_url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:16:19.900116Z",
     "start_time": "2024-03-14T15:15:36.222617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing for the first question\n",
    "\n",
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=q1_bis)]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}} #The thread_id is the conversation identifier. The memory is indexed by thread_id, so setting a different one will search in another thread_id.\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T13:57:03.422213Z",
     "start_time": "2024-03-26T13:56:08.559132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing for the memory of the first question\n",
    "\n",
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Can you explain your last answer better?\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}} #The thread_id is the conversation identifier. The memory is indexed by thread_id, so setting a different one will search in another thread_id.\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T13:44:28.483550Z",
     "start_time": "2024-03-15T13:40:57.907843Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=q7)]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:59:48.976995Z",
     "start_time": "2024-03-14T14:56:38.706357Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=q2)]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T10:51:28.950749Z",
     "start_time": "2024-03-15T10:50:13.179626Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=q2)]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T14:21:50.467383Z",
     "start_time": "2024-03-15T14:20:51.287785Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=q6)]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=q3)]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_stream(app, q2):\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    try:\n",
    "        # Iterate over the stream from app.stream()\n",
    "        for s in app.stream(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(\n",
    "                        content=q2\n",
    "                    )  # Assuming q2 is the content of the message\n",
    "                ]\n",
    "            },\n",
    "            {\"recursion_limit\": 100},  # Additional options for the stream\n",
    "        ):\n",
    "            # Check if \"__end__\" is not in the stream output\n",
    "            if \"__end__\" not in s:\n",
    "                results.append(\n",
    "                    s\n",
    "                )  # Append the stream output to results list instead of printing\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return results  # Return the list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_stream(question):\n",
    "    \"\"\"\n",
    "    Iterates over messages from app.stream(), printing each message until an \"__end__\" flag is encountered.\n",
    "\n",
    "    :param question: The question to be sent as part of the initial stream request.\n",
    "    \"\"\"\n",
    "    # Define the parameters for app.stream() as described\n",
    "    stream_params = {\n",
    "        \"messages\": [{\"content\": question}],\n",
    "    }\n",
    "    stream_options = {\"recursion_limit\": 100}\n",
    "\n",
    "    try:\n",
    "        for s in app.stream(stream_params, stream_options):\n",
    "            if \"__end__\" not in s:\n",
    "                print(s)\n",
    "                print(\"----\")\n",
    "            else:\n",
    "                break  # Exit the loop if \"__end__\" is encountered\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during streaming: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
